QUENTIN (00:00:00):
Today we are going to discuss The Impact of AI on Ethical Questions with Dr. Dorothy Bauer. She's an expert advising decision makers in tech and finance on ethics, sustainability and responsibility. With this mix of academic and business knowledge, she's advising companies and leaders to align financial goals and ethical values. Hi Dorothy, It's a huge pleasure to have you today to discuss this very important question and actually of the AI impact, but actually all, all the ethical question it raised and the ethical problems that are implied by this current revolution that we are living. And I wanted to start with this really this paper that has been discussed widely in the news and in social media, published by Apple employees, called the Illusion of Thinking, which basically point directly to one of your arguments that's overhyping AI is an ethical problem.

QUENTIN (00:01:00):
And I wanted to hear what you think about all this and what's your point about this overhyping problem?

DOROTHEA (00:01:09):
Hi Kanta, thanks for having me. Excited to be on this podcast. And yes, the Apple paper, like last weekend, suddenly everyone was talking about it and I have to disclaim first. I have not read it yet, but I have read its reception mostly. And so it's kind of taken very well or like very welcomed by those who have always said, wait, what Sam Altman and co are doing is they are hyping generative AI and they are ascribing it qualities that it doesn't have. It will never be able to really reason. And now the Apple paper seems to confirm these criticisms with the tests that they made. And for me, I'm looking at these empirical discussions between, you know, different schools of thought on, you know, how to measure reasoning and guys from different corners of machine learning, etc.

DOROTHEA (00:02:07):
Or like more in favor of neurosymbolic AI, etc. And I, you know, I can't judge the empirical aspects. But as an ethicist, what really interests me is like, would we want to have AI that is capable of reasoning? Why would we want to have that? And why are the people who are developing generative AI or like promoting the transformer models? Why are they so keen on making these promises? And that's what gets me started.

QUENTIN (00:02:41):
So I guess the, the narrative or the argument will be that this reasoning, if we can call it like this, but this step of thinking to solve problems from some type of algorithm developed with, for example, reinforcement learning might help to reach, to solve more complete complex problems. So I'm trying to avoid being more intelligent, but it's, it's, it might help to solve more complex problem like with the game of go, which was basically with this type of training, it's not reasoning, but with this type of reinforcement learning that they achieved a level of go player, virtual go player, higher than the best human. And I guess that's the idea is with this kind of step by step solving of problem, we might be able to solve more complex tasks.

DOROTHEA (00:03:36):
It's very interesting to hear your perspective on that. Because I'm more remote from the field, you're more involved. And for me it's more like the general feeling that they are really obsessed with the idea that we need to beat humans and that humans are not good enough. So like, more. And I'm not a doomsday prophet or anything, but I'm more like, why are they so, you know, insistent on arguing that, you know, we need something that is better than humans and you are taking it down to a much more specific level, which is really helpful for me because it also kind of, you know, brings it back on the ground like, okay, yeah, right, thanks for reminding me that we do have some really complex problems to solve and there it might be helpful.

DOROTHEA (00:04:22):
But until now that I'm talking to you, the messages I get from OpenAI and Amodei, from Anthropic, et cetera is a much bigger narrative of humankind needs to be fundamentally changed, et cetera. And so I'm really glad that you reminded me of the thing. Yeah, we do have complex problems to solve and if AGI or whatever you want to call it, careful with this term can help, that's great. But even then, as you claim, you know, it can solve more complex problems. This, in my opinion, will only work when humans play along because these, this AGI or whatever, this reasoning AI will never be able or should never be able to act on its own.

DOROTHEA (00:05:12):
How do we get people to accept and cooperate if there is such a clear cut case where, okay, he I, whatever it is, delivers a solution or suggestion for solving a problem.

QUENTIN (00:05:25):
Yeah, it's difficult indeed. I mean, we have to filter in to be able to ask the right questions, to prompt well, even if it's evolving and helping us sometimes to frame the right question, but it's most of the time. That was the first podcast actually with Pitson, who is an expert on how to ask questions and wrote an entire book on this. And this should really come from us, the type of question we ask. But we definitely need to be able to filter out and I mean to be able to judge. So we do need our human intelligence to understand when there is something useful. When you have some Risks even if we think about this type of general intelligence. But back to basically this view and often the discussion mainly that I've seen recently by Sam Altman, for example, who indeed seems that it's kind of creating this almost godlike creator.

QUENTIN (00:06:19):
And I think one thing that I thought also when you were speaking at the very beginning, I think a few months after ChatGPT 3.5 release or March 2023, there was this document signed by thousands of experts on AI to say we should stop the progress. And I always felt that it, it sounds a bit like a marketing campaign in the sense it's so good what we are doing that we, you have to stop us. And I, I'm wondering if you, if you remember this and if what you thought about it and what you think today about this type of action is it. Is was it really genuine or was it more like a marketing trick?

DOROTHEA (00:07:00):
Honestly, I struggle to believe that anything or a lot that comes from, you know, these circles is really genuine. It's. It's really hard to tell. There are always like overlapping groups. And I think the sign for the moratorium, I think it was this. There, there were people signing who are not aligned with Sam Altman on other things. And so it's, it's really difficult to, to keep track of who is involved where and who believes what. But I think in general hyping like, oh, we will create super intelligence next month as well as saying like stop it, it's so dangerous. Both is just feeding into the hype and it's not helpful. It's like black or white. And this like the.

DOROTHEA (00:07:46):
If Sam Altman even, or guys know that are like related to him or work on similar things, if they said, oh, we are so good now, you need to stop us. This is just, it's dishonest because they have kind of, they claim to have, you know, appropriated all the power to create these things and they put themselves in a position of those who are the only ones capable of keeping it under control. So it's like, you know, adopting a really, an authority that no one has ever given them that's never been legitimized. Insane.

QUENTIN (00:08:24):
Yeah, indeed. And the change, I mean, I'm pretty sure that's one of the person who signed this document was also Elon Musk, who is today trying to lead on the AI realm a few months, maybe one year later. So it's quite, indeed surprising, this kind of opposite view.

DOROTHEA (00:08:42):
Yeah. And even now. So Apple researchers from Apple have published these, this paper, which is another on a side Note, it's interesting that most of the research comes from companies now and there is hardly any independent university research, which also, you know, must make us a bit wary of like, you know, the interests involved. And so, so, so you could also say, oh, Apple, Apple is set to lag behind in AI. Maybe it's in their interest to publish something that says, okay, it's not going to work anyway, so. But I feel like a conspiracy theorist when I say these things. I'm not, I'm not. But the fact is that anything that happens in AI is so distinct from other fields of science, which are, as I said, mostly, you know, cultivated by universities. And now we're having companies with billions of money, billionaires like Musk and Bezos, et cetera, feeding this research, which is never free from their own business interests.

DOROTHEA (00:09:40):
And that's also something we need to take into account. So whose agenda is it? But again, I'm not talking about conspiracy theory. It's just very different from, I don't know, research in chemistry or in physics as we know it.

QUENTIN (00:09:55):
We have seen during the inauguration of the second term of Donald Trump, we have this row of top tech leaders just behind him and it set the tone and on the role of tech leaders today and due to the sheer amount of money they have, but also the impact they might have on society. And also here I'm curious about your knowledge on ethical aspects of these questions because it gives a lot massive power to private companies to shift, for example, or shape the public opinion from managing the algorithm behind X, from Meta, but also on ChatGPT because many people, hundreds of millions are using it probably daily. I'm not entirely sure about the numbers, but a vast, basically set of people are using this daily.

QUENTIN (00:10:45):
And you can just shape the way the tool will answer. You can also play with the training sets and erase some part of history if you would like to. You can also push the opinion in very pervasive way in some direction. And I'm just curious if this is also something you discuss or you see as a risk and how we should deal with the fact that private company have today such an important impact on society.

DOROTHEA (00:11:15):
Yeah, it's certainly something to worry about because I mean, ever since 1990s we've talked about corporations as political actors in business ethics. And now, now no one, now it's not such a popular term anymore, but it seems to be just take for granted. It's, it's become normal. And of course they have a lot of power. You know, also the fact that they already have Billions and they're spending billions directly into politics and into elections, etc. That's like their first influence. And then of course with things like Starlink like Musk, he can switch off, you know, connection wherever he wants. So these are the obvious things. But then again, I think it's like of course they have most power, but everyone can use generative AI, especially like large language models like ChatGPT in order to try to manipulate people.

DOROTHEA (00:12:13):
We've had this scandal recently with this infamous study, unfortunately from University of Zurich, which is nearby, where, you know, researchers tried to undercover feed ChatGPT into Reddit, was it, I think discussion threads and try to convince people and change their opinion. So I think it's very tempting for Anyone to use ChatGPT to their own benefit. Of course the ones who created it, they have most power over it. But it's in general, it's a highly manipulative tool in many ways. For better or worse, you know, you can say I have a good agenda. It's okay, it's legitimate to try to convince people to be less racist or whatever, but it's a fact that everyone can use it however they like.

QUENTIN (00:12:59):
Yeah, so that was exactly my reaction, I think actually in the first podcast where we, I started with this paper I, I would say published in Science, which showed that you could change the belief in conspiracy stories of people on long term. It was really long, lasting at least a few months. What was meant by long here and by discussing with these chatbots that were tuned to basically help to shift the belief of people. And my first reaction was like, you could do the opposite, right? Because it's very convincing, it's always available, it's infinitely patient and it has this inherent way of basically being very convincing. And also because it has been trained on correct answers, basically it was fed with this is a question, this is the correct answer.

QUENTIN (00:13:51):
So then basically these tools learned extensively to be convincing and to pretend that they know. Not only because humans often are over optimistic, over enthusiastic, heroic with their knowledge, but also because specifically in the training set and the way it was trained, it was fed with a lot of correct answers by experts. So if you just show this and as these tools are mimicking, and when I say this, it's really LLMs are mimicking our behavior. They mostly observed people who were confident of their answers. So then basically you have a lot of ways to reduce these issues and they are work on this and they are trying to do this also with web search and other techniques. But, but it tends to, to be very convincing.

QUENTIN (00:14:36):
And you see that indeed many people who are now exposed to this just take it for granted because I know.

DOROTHEA (00:14:43):
But some people think they're like the benevolent dictators that never existed and they are allowed because for good. And that's. It's just plain wrong. But also what's striking, as you say, I don't know. That's not something that he has ever learned. And it never, for example, has like the three dots for like thinking, you know, it pretends to think. It's like I'm thinking now, reasoning, but it's really. And it's, it's annoying because it's also been trained to be very, how they say, sycophantic, like people pleasing, et cetera. But that's why we in some situations prefer it over people because other people don't please us all the time. So. But these are psychological issues. But you know, what I find particularly striking about the fact that these, like AI companies have so much power is that at the same time they claim to be democratizing something.

DOROTHEA (00:15:34):
They say we're democratizing AI, et cetera. They are playing with the word democracy when they in fact are undermining democracy in so many ways.

QUENTIN (00:15:45):
Will you say are the main ways they affect democracy?

DOROTHEA (00:15:49):
Well, as we said, you know, be it by directly, you know, spending money on elections, like by trading government agencies that have never been really legitimized by, you know, training the algorithms and, you know, deciding who gets to publish what on their social media platforms by just making these tools available that, as we just discussed, give anyone, you know, huge opportunities to do their, to influence others, etc. So in many, many ways. But, yeah, so. But at the same time, we're democratizing knowledge, we are democratizing creativity, we are democratizing art, we're democratizing education.

DOROTHEA (00:16:37):
What do you mean? You're just commodifying things on a large scale. You're, you know, selling your products. You're just talking about access in terms of quantity and you're not thinking ahead of what it really means and on whose back. You're basically building up this axis for others. So, yeah, that's one of my pet peeves with the whole, you know, training history of all these models. Like, what were they trained on? Where did they take that stuff from? Who got compensated? Wait, no one. And so why do you call democratization of art now? When she stole it?

QUENTIN (00:17:20):
Basically, yeah, this is you. No, no, but this is a very important one. And there was this very good framing in a TED talk recently, but by Ed Newton Rex on copyright Basically, and.

DOROTHEA (00:17:33):
He says, one of my favorites.

QUENTIN (00:17:35):
Yeah, it's very.

DOROTHEA (00:17:36):
I haven't seen the TED Talk, but I really appreciate. Yeah.

QUENTIN (00:17:40):
And the, basically the. So for the audience, the, basically the pitch in the beginning of this, of this TED was these big AI companies, they have, they require brains or engineers, a lot of data and a compute and they are ready to spend hundreds of millions under engineers, sometimes billions on the compute. So the computers. And for some reason for the third data, they expected to get it for free. And you have this. Recently they said, I can't remember who said that, but basically that they used hundreds, thousands of books under copyright because they had no economic values.

QUENTIN (00:18:26):
But if you used it because it.

DOROTHEA (00:18:28):
Has a value for you, it's so cynical. It's so cynical. So Nick Clegg, the former British politician, then gone Meta executive, he recently said, oh, if we had to compensate all the authors for the work that we use to train our AI models, then the AI industry would go bankrupt. So I mean, I mean, how dare you. So does this legitimize it? And, and it's just, it's. I mean really, I. That Meta used this Russian pirate database, Libchen called all, you know, millions and millions of books and texts to train their llama. Their model. And there was a piece in the Atlantic where you could check whether on that paratized database, you know, what works were in there.

DOROTHEA (00:19:21):
So I entered my name and I realized my thesis, every single article that I've ever published is in that database. So maybe I should feel flattered that maybe Meta used my work. So finally someone read it their LLM to feed their. Their AI. But it's just, it's outrageous what lengths they're willing to go to in order to, you know, expand their power. Unbelievable.

QUENTIN (00:19:47):
It makes sense. For me at least now it feels that with everything we are saying, I might understand why they want to pretend it's like they are creating some superhuman intelligence. Because if it's for the greater good, if it's for this big mission, you basically, it's.

DOROTHEA (00:20:09):
A greater good to democratizing, universalizing, whatever agenda. And then this is. Okay. And this is just pure rhetoric. It's really, it's totally dishonest. Yeah.

QUENTIN (00:20:18):
So it could be very provocative. It could be like some people who use religion for this. Like, it's not me, it's God and it allows me to do whatever and it's for the greater good, for humanity. And yeah, it's.

DOROTHEA (00:20:32):
But it's also, you know, as we know, they don't even make money off it now of their LLMs, it's really hard to make money with them and doesn't pay off now. So a lot of that talk is also actually directed at investors to keep investors interested. So that's also one of the problems when I said like research is being done by the companies themselves. You know, a lot of the research that's published is trying to please investors. And you need to talk up AI and make AI sound better than it is. And at the same time what they also do is they talk down humans, they reduce us to computers. So they say, oh, the human brain is only just a computer anyway. We can easily copy it so we creating something better.

DOROTHEA (00:21:17):
So I think it's both ways. Talking up the skills of AI and playing down the skills or the complexity of humans. And it goes together to, to create this agenda that makes people think like oh, they are our saviors, they're working for a greater good.

QUENTIN (00:21:35):
And even the terminology in AI might be problematic with that respect because the fact that we use neurons. Exactly. Because it has in some way initially the. The concept of mimicking the way our brain works. But we know that it's widely different, it's extensively different. And all the terminology, I mean if you are in statistics, it's not really exciting the terminology. It's really terms that are. There is nothing fancy about this terminology, whatever. And in artificial intelligence it tends to have really exciting terminology like already artificial intelligence neurons, deep neural networks and all these things might also hides or makes us fantasize about what, what's behind this, this math.

QUENTIN (00:22:23):
Because these are just matrix, completion, multiplication. It's, it's really. But it's, it's, it's. Anyway, even if you can reduce it to some mathematical operation, it's still very impressive what it's capable of today. Even if it has cost and implication and issues. You also have some, some very surprising effect and indeed I mean also related to ethical issues. I always seen this situation as a world's natural experiment. It's really like we are not sure what these tools are capable of, but play with them and here they are and we'll see where it goes. And I feel that it's also. I don't know how it's viewed from an ethical perspective from someone expert in this, but it feels like there's, there's clearly some.

QUENTIN (00:23:13):
It's a recipe for some dangers.

DOROTHEA (00:23:16):
Let's say it is I think also what makes. I mean, and I must also emphasize I appreciate AI. I appreciate, you know, it in all kinds of forms. The ones I Notice and actively use and the other ones that kind of guide me, you know, without me knowing it. I mean, I'm totally not anti AI and you know, really think there's a lot of good to be gained from it. But as you say, one of the problems in my eyes is also that we are always talking about general purpose AI. So that already says like, okay, we created something that channel purpose, so up to you what you use it for. So we are not responsible for it. It's general purpose. You know, it's like whether you use a hammer to hit the nail or to kill someone, it's your responsibility.

DOROTHEA (00:24:05):
But this channel purpose also, like framework and, and rhetoric is really just escaping accountability or like passing on responsibility to those who use it. But imagine if maybe it's a stupid comparison, but if nuclear technology was general purpose and anyone could just use it, you know, you wouldn't. And of course I'm not comparing the direct dangers of generative AI with nuclear technology, but it's just like this general purpose mindset that you're releasing something that's so powerful under the label or with the frame general purpose should not be ignored.

QUENTIN (00:24:49):
So this leads also to this basically confusion that we have today. So I was giving a talk recently to C Suites about AI and different type of AI and I realized that for some people, even if I did my best to explain what I was mentioning by AI, for some at the very end they thought where can I use this predictive AI? Basically so things that we have been doing for long time, decades in companies that these issues are working, they all pictured basically AI as some kind of chatgpt with different functionalities. But it is one specific type, right? It's really generative AI and large language models in particular. So you mentioned this also, if I'm not mistaken several times, that this, that this also creates issues when we are not clear about what we are discussing.

QUENTIN (00:25:38):
Is it AI for prediction, for forecasting in financial sector, for example, to detect and anticipate fraud, or is it Genai, which is very different?

DOROTHEA (00:25:51):
Yeah, it's in many ways it's really important that we separate these because on the one hand, just be aware that AI is more than just generative AI and that the ethical issues that are related to generative AI are different from the ones like risks and opportunities from the ones in predictive AI. But it's this generative AI discourse is just dominating everything. And I've also seen a CEO of one of the big four consulting companies in, in Switzerland, when he was asked on a Panel like, so how does AI play into your, you know, role as a CEO of this big company? He said, oh, I, I have an LLM that helps me write emails and like, if this is like one of, you know, the big four, if this is what comes into the mind of one of the CEOs, one of big four, it's, it's really a bit worrying.

DOROTHEA (00:26:45):
But I think also. So on the one hand generative AI has its problems as we have discussed its general purpose. It's, it's, you know, manipulative potential manipulative, potentially biased, can also, you know, create access for people to tools that they didn't have, etc. For the, you know, on a positive note. But predictive or AI for example, is single purpose. Often that's used for a specific task. For example, when it comes to climate science, you're predicting weather events or analyzing, you know, biodiversity, whatever, which is very helpful and which is hands on and which is less energy intense and which in the case of biodiversity doesn't hurt anyone's privacy or data hasn't been, you know, stolen mostly I hope.

DOROTHEA (00:27:34):
And so we're missing all the opportunities. So also for those who are like the doomsday profits when it comes to AI, they only focus on, on generative or like AGI promises, etc. But there is a lot of power like positive potential in AI, but maybe more so in single purpose AI where we specifically, you know, develop and use an AI to application to, to address specific problem in just of just instead of just, you know, giving away the general purpose AI and then see what happens. So I think in many ways it's, it's, it's, it's not good that this, this lopsided focus on generative AI and also I appreciate a lot.

DOROTHEA (00:28:20):
One of the best books I've read recently is AI Snake from Arvind Narayanan and Kapoor where they show so many use cases. Of course they look at them from a critical perspective. But that's also important on, you know, what can you really predict from data? How can human behavior be predicted? What is possible, what is not possible? Where can we rely on, you know, machine learning or AI to help us and where are the boundaries and this. All these aspects get bypassed by everyone talking about their latest prompts.

QUENTIN (00:28:54):
It's true that while you are saying this, I could have this image of how everybody was shocked and how it was relayed and everybody tried and also because it was accessible, I guess. But with ChatGPT 3.5 everybody was amazed that it could write Some kind of poems. For example, quickly, while you had AI before helping to save lives, for example, by helping to detect tumor with MRI scan.

DOROTHEA (00:29:22):
Exactly.

QUENTIN (00:29:23):
And this is like not so, so popular or so well known by many people. So.

DOROTHEA (00:29:29):
But even then, for example, when it comes to, you know, image recognition, radiology, etc. It's so important to use AI in that context. But then you have the people who immediately claim we will not need radiologists anymore. They immediately impose themselves on people and. Or like, yeah. Then they claim they want to do away with the humans, which then triggers all the critics who focus on the cases where AI has not recognized the tumor instead of combining the two, you know, okay, so we have AI that can help us with image recognition in medical settings, but we still need humans. That's clear. Because all the benchmarks that they create to measure how well an AI performs in radiology are totally taken out of context.

DOROTHEA (00:30:15):
They are not applicable to the everyday work of radiologists anyway. But it's, it's like such a talk, sorry, toxic culture. So the ones say, oh, we have AI now, go away radiologists, we don't need you anymore. And the others who say, like, it should never be used, it must never be used, etc. It's too dangerous. And why can't you see it as a complementary relation? So it's really striking.

QUENTIN (00:30:41):
Yeah. They are rarely the case that. So in some situations, automation most of the time might be augmentation. Indeed. I mean, that's one of the, the, the type of post I see the most on LinkedIn that is driving me crazy is reap. And you say some kind of, of work, like rest in peace. Legal.

DOROTHEA (00:31:02):
Yeah. Oh, okay.

QUENTIN (00:31:03):
Developers.

DOROTHEA (00:31:04):
Yes.

QUENTIN (00:31:05):
And you're like, well, you know, so far it's not what we are seeing. It's helping to augment if you do it well. But sure, there are some job destruction. I mean, you.

DOROTHEA (00:31:15):
Yeah, sure, but that's what I'm saying. Why do they feel so obsessed with playing up or talking up the skills of AI and playing down the skills of humans? Why, why is it, why can't you just say, hey, we have a helpful tool here, humans, you can do your jobs better. And of course some jobs will, you know, be become obsolete as always in history. But there's like either the machine or the human. It's just not helpful.

QUENTIN (00:31:43):
But interestingly, I see some people who start to realize the value of humans due to this substitution that some people are pushing and they're like, actually, I really like to exchange with a human to learn from A new human to be in a room with a physical person with their limitations, physical experience of the world and emotions and suddenly we realize that this or we some people, it helps them to realize that actually the all the uniqueness and valuable things that humans have to offer. But indeed on the other hand of the spectrum, I guess you have other people who for some reason would like to automate, replace everything and does this doesn't seem to have any answer.

QUENTIN (00:32:28):
What, what will be then the place of humans in this society. Which is, which is peculiar.

DOROTHEA (00:32:34):
Yeah. Yes.

QUENTIN (00:32:36):
So when, when you go and talk to leaders in the business world, how do you see that this hype. How do you deal with this hype and basically help them to basically see it where, how they should see it or what are the implication you. You see from this hyping of AI in the business world with leaders?

DOROTHEA (00:33:00):
Well, I mean I'm mostly you know, teaching in continuing education like for example AI and ethics in a, in a class on AI and finance. So there for example I developed a credit worthiness game with algorithms and, and I played with different notions of fairness just to show them the implications. If you think like you have an algorithm that's deciding who gets a credit and who does not. And then I kind of play around with like the, the normative or like. Yeah, normative like moral assumptions that are underlying different conceptions of fairness. So I always keep it on a very appl in my teaching and in my keynotes. Yeah, it's mostly kind of tongue in cheek also like showing that this is what's possible and wait here there's something here.

DOROTHEA (00:33:49):
I'm always ending with a strong plea for the human element must be kept in loop. But of course, I mean with people who are like totally determined to hype something or who have also as I say skin skin in the game, like you know, why would I want to ruin their dreams? But they don't get in touch with me anyway. So that's you know, not meeting those too often. But yeah, in continuing education you have people from different companies and you can see, you know, different levels of literacy. And I just always work with cases and case studies and games etc. To kind of reduce the distance between them and the algorithm to see, you know, what happens when you use AI in such and such setting.

QUENTIN (00:34:41):
I see, I see. Yeah, interesting. And then indeed I've read some research also to try to see when you don't you let just these also Gen AI tool play traditional games we used in sociology and economics to see what are Their traits, basically their behavior. And it was rather interesting to see their standard answer and what it has learned basically. And of course you can tweak it, but it's interesting to see already the same standard answer. And based as you discussed about teaching, that's also a huge topic. I mean I have also many students every year and young professionals and, and I mean it's, it has serious impact on the way people learn. But also mainly something I wanted to discuss with you about this basically democratization, inequalities and the fact that you, you have some research showing well in these developing countries where it's hard to have the resources, we can automatize the teacher or augment the teacher or replace the teacher entirely with an AI tool.

QUENTIN (00:35:45):
It might help to give access to education to many people. And in this discussion as well as in other topics like mental health, I'm always. I see the benefits because I see some research that I find convincing where I see that it can help if it's well made and well done. But in other situation I'm a bit also afraid that then we might say to the poorer you'll get the AI and to the richer you'll get the real experts who can afford a human. Exactly.

DOROTHEA (00:36:20):
Same doctors. You know, if you don't have a doctor in your village and you have AI based medicine, of course it's like better than nothing. But. Yeah, yeah. But I must say I'm careful with the whole education context because I'm only a guest lecturer. Luckily at universities I'm not involved in exams so I can't really say anything or I'm lucky not to have to deal with the fact how do we make meaningful exams when everyone is using ChatGPT, etc. And, and, and the psychological questions about mental health is also something that yeah, I'm not really qualified to say but as you say like the value of the human and as you said before, it's, it's a paradoxical effect that the more we get to see AI, maybe the more distinctive is the value of the human interaction that we still have.

DOROTHEA (00:37:12):
But as to for example democratizing access to education, I mean the first thing is you have public libraries, create public libraries. You don't need an AI generated teaching assistant, whatever. Why does it always take AI? I think it's taking me too far. We can start with other things like people need to have access to books and to also information, not just on TikTok or on social media. And that's where we need to start before we start thinking about creating A specific AI teacher for some remote regions of the world.

QUENTIN (00:37:59):
And indeed so some people, I suspect might react when they here today, somebody mentioning a library that's, well, you have a lot of. It's not efficient. You have to go there, you have to look yourself within something. You have to open a book, find the right pages. The thing is with learning, and I was also doing this episode on the podcast with MIT Prof. Who is focused on exactly these questions we were discussing. And he was raising the point that you need some friction to learn. You need those basic. You cannot accelerate everything. You have to spend time and wrestle with the thing and actually maybe looking for a page, seeing something is other book chapters that you didn't expect expose you to new things, make you learn better and basically acquire better knowledge.

QUENTIN (00:38:50):
And he did this very nice experiment where he gave to his students to solve a problem. ChatGPT other has a low, lower level. It was code llama, basically lower level AI Other has traditional, traditional resources. Everybody could solve the task. The one with ChatGPT the fastest. And the day after he, he said this in the beginning, you'll have an exam on this topic tomorrow. And the one who used ChatGPT had the worst grade. And because basically they could, even if they knew they, they will have an exam, it's because they took these shortcuts and they could not compensate with these tools to properly learn. So these things in the physical world are in more traditional ways of learning, are very useful.

DOROTHEA (00:39:34):
You need to go through the process. I just quote from a blog article from Nicholas Carr that I read recently on like the myth of automated learning, you cannot automate learning. So he says AI is used by high school and college students to complete written assignments to ease or avoid the work of reading and writing. Puts the process of deskilling at education's core. To automate learning is to subvert learning. It's a brilliant piece. I mean, I wish I could read it from A to Z now, here, because the way he nails it, you cannot delegate learning, you cannot outsource learning, you cannot automate it. If you haven't done it yourself and your brain has understood the whole thing, you're not going to know it. And that's why also when you know generative AI came first, it was like, now everyone can be a writer, everyone can be a photographer or an artist or a composer, etc.

DOROTHEA (00:40:28):
But. And it was seen as creating more equality. But it's different. Like those people who have already been authors, they become even better because they already know, you know, they know their skills and they can use it to become even better. But people who have never written a text, who don't know what a good text is, they won't be able to judge the output that the AI serves them. And the same with pictures. I mean, I will not become an artist just because suddenly I would have access to a Danny whatever because I don't know art. So in the end it's like keeping those who have never learned the skills and if they're never taught the skills anymore, because we don't invest into human teachers and into the process of acquiring skills, it keeps them further away and it blocks them from entering the whole world of art or, you know, writing etc, Whereas those who had already, who had the skills already, the moment that generative AI popped up, they're just moving further ahead and it's really driving people apart.

QUENTIN (00:41:34):
Yeah. So that's really something that I fear with my students. Indeed it's seeing this, I mean, if I've learned to code and I used to do it for the last 20 years, I can now benefit from these tools because I can code faster. Just reread, check, adapt quickly. But then it's very hard to learn if you already start with this. And same with many skills. I mean my. So somebody told me recently that they were writing an email in front of maybe someone who were 20 years old something and he was surprised. The person was like, but do you still write email yourself? And others will say what's that? I've heard many, many times these days. Also, why do you use Google? What?

QUENTIN (00:42:20):
What's the purpose of using Google? It's really interesting because it, it shows how fast the world changed and indeed it, it raised many questions on. On what? And that's really my perspective today when I teach because I spend a lot of time I was doing experimenting but also using AI and hopefully in some way that I, I can indeed augment my capacity and not reduce it. Also with learning, when I use specific tools like NotebookLM to just interact with the content in an efficient way, but still using it like a tutor or a sparring partner to acquire the knowledge rather than spitting the answers. And the thing is, what I always share is as every time when I use AI, I try to do it consciously.

QUENTIN (00:43:05):
I know that I might gain some productivity time, at least on the short run. But almost inevitably I will lose something.

DOROTHEA (00:43:15):
Yes.

QUENTIN (00:43:15):
And I have to accept this and make the conscious decision, okay, I'm ready to lose on the learning capacity on this to gain a bit time to focus on something that I give more value but. But this trade off is not always clear to everyone and I think it's important to take this into account before interacting.

DOROTHEA (00:43:37):
Yeah, I think it's a big advantage of those who know both worlds because we can compare. We see what we gain, we see what we lose. But there is also this famous saying like every augmentation is an amputation. I think it just applies, you know, I mean like when you simple things if you only use Google Maps you're not able to read a map anymore, you know how you navigate etc. And this also applies here just that there is more at stake than just reading a map or using Google Maps. There is what is at stake is like as we said, like the way of like proper thinking and acquiring knowledge etc. That's what's at stake. So that's. Yeah, but that's why I'm really glad I'm not involved in. Not full time involved in universities et cetera because these questions are really tough to handle.

QUENTIN (00:44:25):
They're very hard. Constantly discuss how to evaluate how to teach. I constantly try in every class now I teach basically generative AI because it's. People are using it so so that's. Anyway yeah try to help it Was this the also the starting point with the episode on mental health? The person who created an app specialized to support mental health in the university basically based on gen AI he said well basically one of and it was a result from a recent research survey. The first way people use these gen AI tools recently has been for companionship and mental health support. So in a way basically people use it for this. So let's at least give an obvious choice that is adequate or at least to the best extent has some safeguards and can tweak, push help bring something to the table redirect people to specialist humans specialists and and still try to take the advantages from these tools.

QUENTIN (00:45:28):
For example, in this the case of mental health we were discussing what happens if it's 2am and you cannot reach to anyone and it's just to. You are in a stressful situation and you have an adequate tool that can help you with the right training to reduce the. The. The mental loads and redirect to. To. To the. The professional. So in some situation it can definitely at least I think help if you take the advantages. But we have to be very clear that we almost always lose something.

DOROTHEA (00:46:06):
Yeah, we do, we do.

QUENTIN (00:46:08):
So we discuss very quickly about something but again maybe I'm not sure how much time you spent discussing and thinking about this. But about the copyright, which is very close to the. The ethical questions, right?

DOROTHEA (00:46:23):
Yes.

QUENTIN (00:46:23):
We have this basically Ghibli Giblification, and it was something very dear to my heart because I was a big, big fan of Ghibli and I grew up with these movies. And some people were telling me, well, you know, it's, it's. It's doing some advertising, some marketing for AO Miyazaki, because I didn't know him.

DOROTHEA (00:46:43):
Before, and not what he wants, not the type of advertising he wishes.

QUENTIN (00:46:49):
Yeah, exactly. So that was first, also my, my reaction. Second the second, I was always also telling these people. So at point taken. Let's say for Ayao Miyazaki, it's some kind of advertisements. It increased the number of people who watch these movies. Recently I convinced that he would and probably hated it. But let's not think that I know the answer. But for all the other artists where you cannot identify the art, it's not helping. And this is the vast majority. So you're already helping the people on top. So. Yeah, I don't know what you think about this could be the fix.

DOROTHEA (00:47:31):
Yeah. No, no, no, no, no, no. Please don't expect the solution from me. Just today I'm submitting an article for a Swiss art magazine on, on AI and art and copyright and democratization of art, etc. Because I gave a keynote last year at the Institute for Intellectual Property in Switzerland on AI, creativity and ethics. And so they liked it. They said, oh, can we have that, you know, kind of an article on We Need Beer magazine? And we, we want to hear your voice in the magazine. So. Because there, you know, copyright is a, is a, is a very legal issue where. No, I'm not a lawyer. That's the first thing to say. But the thing is that as we mentioned before, they have stolen everything, all the works that they could find, whether in violation of copyright or just like in violation of moral decency.

DOROTHEA (00:48:28):
That's another question. And then now they're creating tools that are actually designed in the most cases, to actively hide the authority. They're designed to hide the author. They're designed to avoid attribution, they're designed to avoid having to compensate. So in the Ghibli case, it was like in your face, straightforward plagiarism that they were able to train, like, copy this style, which is an exception because usually what we get is like actively try not to look, plagiarize too much and dissolve all the traces of the individual creativity that you have been trained on. And that's the very, very unfair.

DOROTHEA (00:49:13):
So and, and compensation models. I don't know if, if I hope that there are so many now, there are so many court cases going on against this practice. I hope that there will be some verdicts where companies are paid, at least in hindsight or forced to pay in hindsight to the creators whose works they have taken. But it's. It's like after the fact, first they stole it and now you know, you cannot undo the theft. It's impossible.

QUENTIN (00:49:45):
Yeah, indeed. It's already this. And even if you want to, basically there is this very important topic and very technical one which is called machine unlearning. So it's basically you try to make the machine forget something, it learns, but it's very hard because typically with cutting edge models where you have more than 1 trillion parameters, it's very hard to understand where this is embedded, what's it learned and to make it forget. So it's a huge topic now in the engineering basically world, it's like a.

DOROTHEA (00:50:18):
Data leakage, privacy, you cannot undo it. It's out there. And now everything has been fed into these models. And now after this has been done, you want to change the reality. It's not possible.

QUENTIN (00:50:34):
So one thing that I wanted to note that I really appreciate while talking to you, which also highlight the difference with ll, is that several, several times you note exactly where is your expertise and when you don't want to go, I'm not a lawyer, I'm not lawyer.

DOROTHEA (00:50:49):
I haven't read the paper, I'm not a lawyer, I'm not a machine learning expert. Yeah, I know my boundaries, thanks.

QUENTIN (00:50:54):
But that's usually. Interestingly, that's something I discuss a lot over the years. But I always see people and perceive people more highly instead of the opposite when I hear that they openly share their limitations, the things they don't know because we don't know everything. And the fact that you're able to know your boundaries and also to say it out loud shows that you know that you don't know and your ego is not too big such that you're able to say it loudly. Well, this I don't know. I let it to the expert to judge. I might have an opinion.

DOROTHEA (00:51:31):
But no, thanks for mentioning that. And I'm always. I also care about attribution. You know, I want to say where I learned something from because that's helpful for other people to know. I want to say I'm reading from this blog now. I say, you know, I'm reading AI Snake Oil because that's also what differentiates me from an LLM unless it's rag and it's correct. But you know, I said like, hey, I read this in that book. Maybe you want to read that book as well or read that blog piece. And that's what makes us humans. We learn from humans. And I think it's it really. Every single blog piece or article that I read online stays. So much brains ceremons I get from ChatGPT. Even if I only want to know, you know, something brief, it doesn't stick because there is no human author behind it.

DOROTHEA (00:52:22):
And I really think there is a difference in reception. When I read a carefully crafted blog post or book book written by humans, it stays because there is a history, there's a story, there's an author, there is experience behind it. Whereas whatever vomit comes through chat on the screen, it's just, you know, I say stochastic, not stochastic gambling. Yeah. Which is plausible, but there's no authenticity.

QUENTIN (00:52:54):
Yeah, I really like this, this point about indeed you connect better due to the history behind it. Really also relate to the many discussion I had about art where people were telling me well, but if you see a piece of art and you cannot say that it's AI generated and you find it beautiful, would it be really different that if it's made by a human? It is. It is.

DOROTHEA (00:53:17):
Imagine you read, you read a great book, your favorite novel, and then you notice it has been written by Claude. You will be so disappointed because you had ideas in your mind about the author while you were reading it. You were creating, you know, assumptions, you know, explicitly or implicitly, and then you find out there is no author behind it. There is no story. It's. It's frustrating, it's. It's disillusioning. It's. I call it the disenchantment of, of art through AI.

QUENTIN (00:53:51):
No. So I 100% agree. Some people disagree and I had many debate about this, but it's a very interesting debate. But it feels that I have been discussing with you for five minutes and I see that the time is moving.

DOROTHEA (00:54:05):
Yes, it's running.

QUENTIN (00:54:06):
There is a huge topic I wanted to discuss which is the impact on sustainability and environmental questions. And I'm very curious because so far I failed to find someone to come on the podcast and discuss this. It's a very tough one. So I've been also working on sustainability before moving away from academia and measuring environmental impact is difficult for many reasons. But first I'm curious about what your view on this in light, for example, of this recent estimation, which is what it's worth from Sam Altman on his blog mentioning that basically an average request on ChatGPT has a low use, very low energy, more or less, I'm not sure now, but one minute of the oven, that is all these comparisons, like, yeah, some, something drive your car.

DOROTHEA (00:55:02):
It's worse than asking ChatGPT if you, you know, whatever. All the, like, comparing Apple and oranges is like one of the favorite disciplines of people in the field. Yeah. I must say, you know, again, I'm not an engineer. So now Sam Altman said it's, it's 0.37 kilowatts, you know, one query. But then again, you know, the methodological discussions start, oh, this is only for text. He didn't talk about image generation. And, and you know what, if you multiply it by billions of times, of course you can say, oh, One query with ChatGPT is much, much better or less worse than driving my car 10 kilometers. But, you know, it's, it's a scale that accounts for it. I think the problem is that the coincidence, the fact that at a time when, let's say in 2020 or like, yeah, when the pandemic hit, et cetera, we were on a path where there seemed to be some consensus that, you know, climate change is an issue that we should tackle and we need, you know, become more energy efficient, we need to have renewable energies.

DOROTHEA (00:56:06):
And then suddenly several things happen. One of them being generative AI pops up and is so energy intense that it threatens the transformation, the energy transformation. And that generative AI or transform models, like the big AI models, they are responsible for data centers in areas with water stress, where the data centers consume a lot of water and a lot of electricity, etc. And that's just. It couldn't have come at a worse time because just at a time when we kind of agreed that, you know, we should move towards renewables, et cetera, this pops up. And it's also very interesting from business ethics point of view, to see the change in the tech industry, as you said at the beginning of this podcast, how the CEOs from Big Tech lined up at Trump's inauguration a few years before.

DOROTHEA (00:57:03):
They were like, saying things like, Microsoft would say, we are becoming carbon negative by 2030, we will remove all historical emissions since 1975. And then they partially bought OpenAI, and now they're falling behind their climate targets. And what for, you know, if, if the kind of AI that OpenAI develops at least was the same kind of AI that can help us tackle climate change issues then it would be fair. But the kind of AI that OpenAI develops is mostly used for Ghibli fraud, for you know, fake chat or fake cat videos or deep fake porn, whatever, you know, that's consuming energy. Whereas potential solutions to climate change issues probably come from far less energy intense models.

DOROTHEA (00:57:51):
And this is, I don't know, I can't get my head around that and how we try to absolve ourselves from the responsibility one query only uses, you know that and that much and well, I won't use my car today so I can spend my day on chat GPT with a good conscience. That's just ridiculous. It's. We really need to think about that.

QUENTIN (00:58:14):
And, and basically you point exactly to the direction why it's also very complex to answer these questions because in the narrative of AGI, one of the arguments connected to sustainability is the following. What if it can helps to solve cancer or, or climate change? Because you find the solutions with these tools, what is the cost you're ready to pay to basically solve these issues? Which is. It's a very difficult question to to answer because you have a lot of uncertainty the capacity of these tools to reach this goal and seconds when and if, if it will be capable of doing this. So then you investing with a lot of uncertainty.

QUENTIN (00:59:01):
But that being said, it's still true that we use these tools in scientific research extensively. If it's helping or not, we can leave it open here and hopefully it has at least both effects. In some situation might help, in other it might be destructive. But then basically it leads to the right concept comparison and I think that's the hardest because indeed you create some new needs and energy spending like giblification. Everybody create these images that are energy consuming using water and that maybe people who have done other activities less harmful to the environment without I'm not sure. But when it comes to work and also how we substitute some actions and gain in some situation productivity, we still have this question what will be the static code without Genai, without AI in general?

QUENTIN (00:59:55):
What will be the action the human will take and will it be more or less harmful to the environment because humans are polluting, right? And if I think that I have to, I don't know, write a report doing research on Google, writing and writing and writing on my computer and taking days. If I can be a bit more efficient with Gen AI spending more energy but for shorter amount of time it's difficult for me to wrap my head around this. So that's why I still struggle to fully have a position on this for sure. You see the data centers they suggest to create nuclear facilities next door just to fuel with LGDs. So for sure it's, it's, it's a massive.

QUENTIN (01:00:42):
And I, I agree that reducing to the average cost of a request is really misleading because you have complex, more or less complex requests and also vastly different type of usage. Some that are relatively useless to compared to other that might help to do scientific research. And we can, we can also debate if actually art on no, not art but just joking and create these things are, is useless or not. But well just overall as you can see, I'm a bit puzzled by this question. I'm not sure.

DOROTHEA (01:01:16):
Yeah, same same. I mean I'm hoping, you know, I'm hoping it's true. I'm, I'm hoping that it pays off. I'm hoping that this is, you know, what's going to happen, that we're having a breakthrough. But still, as I said a few minutes ago, it takes humans to cooperate. Even if AI finds a puzzling solution to cancer or you know, climate change, whatever, are we willing to accept that or is the, you know, we also have an oil industry and fossil interests that are not in favor of that. So.

QUENTIN (01:01:44):
Yeah, so what will be your advice at the individual level already for people who want to interact with Genai in a more ethical way?

DOROTHEA (01:01:58):
Use it carefully. Well, I mean what can I say? You can choose the provider. We do have European alternatives now with Lucia like the, the French, France based AI. So if you have some geopolitical issues and if you, you know, care about these topics, you have European alternatives. You know, make sure that you never copy paste, make sure that you, you know, keep your brain in the loop. But you know, as I said in the beginning, it's yeah, all the educational advice is very difficult to give and I'm also still figuring out how to use it best myself. Haven't resolved that yet.

QUENTIN (01:02:45):
And for businesses, will you have also advices or things for businesses?

DOROTHEA (01:02:51):
I think you know, your, the way how you handle AI, where you use it, how you use it is part of your overall non financial responsibility. Just as you address your environmental responsibility, your social responsibility, you have a technological responsibility, make sure you include AI into your materiality assessments. Is this where do we have an impact with our use of AI, what are the risks, etc. You know, just deal with it like one of the other issues that you have already had to incorporate into your non financial responsibility strategies.

QUENTIN (01:03:29):
And one very last question that I love to ask to close the podcast. Are you more hopeful or fearful about the future with AI?

DOROTHEA (01:03:39):
Can I say I'm Swiss, I'm neutral? I don't know. I don't know. It's really difficult because it's on so many levels. On a personal level, I think for, for me, you know, I, I might be one of those who benefit. I don't feel threatened, but on a, on a global scale, it looks different. So I can't really have one answer to that question. It really depends on whose perspectives I might benefit, I guess, or overall, I, I'm not threatened by it, but for other people it looks different. And we, as we said with the Sustainable Development Goals, we said we leave no one behind. And I think, you know, the same has to be, you know, applied to AI. We need to make sure that we don't just cut off people who never had a chance to even acquire skills and leave them behind.

