1
00:00:00,400 --> 00:00:05,440
[QUENTIN]
Today we are going to discuss The Impact of AI on Ethical
Questions with Dr.

2
00:00:05,440 --> 00:00:12,480
[QUENTIN]
Dorothy Bauer. She's an expert advising decision makers in tech
and finance on ethics, sustainability

3
00:00:12,640 --> 00:00:19,360
[QUENTIN]
and responsibility. With this mix of academic and business
knowledge, she's advising companies and leaders

4
00:00:19,440 --> 00:00:25,160
[QUENTIN]
to align financial goals and ethical values. Hi Dorothy, It's a
huge pleasure to have

5
00:00:25,160 --> 00:00:30,080
[QUENTIN]
you today to discuss this very important question and actually of
the AI impact, but

6
00:00:30,080 --> 00:00:35,680
[QUENTIN]
actually all, all the ethical question it raised and the ethical
problems that are implied

7
00:00:35,680 --> 00:00:40,760
[QUENTIN]
by this current revolution that we are living. And I wanted to
start with this

8
00:00:41,000 --> 00:00:46,840
[QUENTIN]
really this paper that has been discussed widely in the news and
in social media,

9
00:00:47,080 --> 00:00:55,040
[QUENTIN]
published by Apple employees, called the Illusion of Thinking,
which basically point directly to one

10
00:00:55,040 --> 00:01:03,390
[QUENTIN]
of your arguments that's overhyping AI is an ethical problem. And
I wanted to hear

11
00:01:03,390 --> 00:01:08,110
[QUENTIN]
what you think about all this and what's your point about this
overhyping problem?

12
00:01:09,070 --> 00:01:14,430
[DOROTHEA]
Hi Kanta, thanks for having me. Excited to be on this podcast.
And yes, the

13
00:01:14,430 --> 00:01:19,750
[DOROTHEA]
Apple paper, like last weekend, suddenly everyone was talking
about it and I have to

14
00:01:19,750 --> 00:01:25,150
[DOROTHEA]
disclaim first. I have not read it yet, but I have read its
reception mostly.

15
00:01:25,790 --> 00:01:32,740
[DOROTHEA]
And so it's kind of taken very well or like very welcomed by
those who

16
00:01:32,740 --> 00:01:38,620
[DOROTHEA]
have always said, wait, what Sam Altman and co are doing is they
are hyping

17
00:01:38,620 --> 00:01:44,460
[DOROTHEA]
generative AI and they are ascribing it qualities that it doesn't
have. It will never

18
00:01:44,460 --> 00:01:49,900
[DOROTHEA]
be able to really reason. And now the Apple paper seems to
confirm these criticisms

19
00:01:50,220 --> 00:01:57,180
[DOROTHEA]
with the tests that they made. And for me, I'm looking at these
empirical discussions

20
00:01:57,180 --> 00:02:03,460
[DOROTHEA]
between, you know, different schools of thought on, you know, how
to measure reasoning and

21
00:02:04,340 --> 00:02:10,700
[DOROTHEA]
guys from different corners of machine learning, etc. Or like
more in favor of neurosymbolic

22
00:02:10,700 --> 00:02:16,820
[DOROTHEA]
AI, etc. And I, you know, I can't judge the empirical aspects.
But as an

23
00:02:16,820 --> 00:02:25,340
[DOROTHEA]
ethicist, what really interests me is like, would we want to have
AI that is

24
00:02:25,340 --> 00:02:30,660
[DOROTHEA]
capable of reasoning? Why would we want to have that? And why are
the people

25
00:02:30,660 --> 00:02:35,740
[DOROTHEA]
who are developing generative AI or like promoting the
transformer models? Why are they so

26
00:02:35,740 --> 00:02:39,580
[DOROTHEA]
keen on making these promises? And that's what gets me started.

27
00:02:41,020 --> 00:02:47,550
[QUENTIN]
So I guess the, the narrative or the argument will be that this
reasoning, if

28
00:02:47,550 --> 00:02:52,150
[QUENTIN]
we can call it like this, but this step of thinking to solve
problems from

29
00:02:52,550 --> 00:02:59,750
[QUENTIN]
some type of algorithm developed with, for example, reinforcement
learning might help to reach, to

30
00:02:59,750 --> 00:03:05,590
[QUENTIN]
solve more complete complex problems. So I'm trying to avoid
being more intelligent, but it's,

31
00:03:05,910 --> 00:03:10,390
[QUENTIN]
it's, it might help to solve more complex problem like with the
game of go,

32
00:03:10,550 --> 00:03:14,630
[QUENTIN]
which was basically with this type of training, it's not
reasoning, but with this type

33
00:03:14,630 --> 00:03:21,610
[QUENTIN]
of reinforcement learning that they achieved a level of go
player, virtual go player, higher

34
00:03:21,610 --> 00:03:28,530
[QUENTIN]
than the best human. And I guess that's the idea is with this
kind of

35
00:03:29,490 --> 00:03:35,570
[QUENTIN]
step by step solving of problem, we might be able to solve more
complex tasks.

36
00:03:36,850 --> 00:03:42,250
[DOROTHEA]
It's very interesting to hear your perspective on that. Because
I'm more remote from the

37
00:03:42,250 --> 00:03:48,480
[DOROTHEA]
field, you're more involved. And for me it's more like the
general feeling that they

38
00:03:48,480 --> 00:03:54,800
[DOROTHEA]
are really obsessed with the idea that we need to beat humans and
that humans

39
00:03:54,800 --> 00:03:59,600
[DOROTHEA]
are not good enough. So like, more. And I'm not a doomsday
prophet or anything,

40
00:03:59,600 --> 00:04:07,000
[DOROTHEA]
but I'm more like, why are they so, you know, insistent on
arguing that, you

41
00:04:07,000 --> 00:04:10,080
[DOROTHEA]
know, we need something that is better than humans and you are
taking it down

42
00:04:10,080 --> 00:04:13,040
[DOROTHEA]
to a much more specific level, which is really helpful for me
because it also

43
00:04:13,040 --> 00:04:18,179
[DOROTHEA]
kind of, you know, brings it back on the ground like, okay, yeah,
right, thanks

44
00:04:18,179 --> 00:04:22,019
[DOROTHEA]
for reminding me that we do have some really complex problems to
solve and there

45
00:04:22,019 --> 00:04:27,379
[DOROTHEA]
it might be helpful. But until now that I'm talking to you, the
messages I

46
00:04:27,379 --> 00:04:36,139
[DOROTHEA]
get from OpenAI and Amodei, from Anthropic, et cetera is a much
bigger narrative of

47
00:04:36,539 --> 00:04:42,939
[DOROTHEA]
humankind needs to be fundamentally changed, et cetera. And so
I'm really glad that you

48
00:04:42,939 --> 00:04:47,179
[DOROTHEA]
reminded me of the thing. Yeah, we do have complex problems to
solve and if

49
00:04:47,179 --> 00:04:52,640
[DOROTHEA]
AGI or whatever you want to call it, careful with this term can
help, that's

50
00:04:52,640 --> 00:04:57,880
[DOROTHEA]
great. But even then, as you claim, you know, it can solve more
complex problems.

51
00:04:58,680 --> 00:05:05,840
[DOROTHEA]
This, in my opinion, will only work when humans play along
because these, this AGI

52
00:05:05,840 --> 00:05:11,360
[DOROTHEA]
or whatever, this reasoning AI will never be able or should never
be able to

53
00:05:11,360 --> 00:05:17,220
[DOROTHEA]
act on its own. How do we get people to accept and cooperate if
there

54
00:05:17,220 --> 00:05:22,420
[DOROTHEA]
is such a clear cut case where, okay, he I, whatever it is,
delivers a

55
00:05:22,420 --> 00:05:25,260
[DOROTHEA]
solution or suggestion for solving a problem.

56
00:05:25,660 --> 00:05:29,900
[QUENTIN]
Yeah, it's difficult indeed. I mean, we have to filter in to be
able to

57
00:05:30,380 --> 00:05:35,540
[QUENTIN]
ask the right questions, to prompt well, even if it's evolving
and helping us sometimes

58
00:05:35,540 --> 00:05:39,020
[QUENTIN]
to frame the right question, but it's most of the time. That was
the first

59
00:05:39,020 --> 00:05:43,900
[QUENTIN]
podcast actually with Pitson, who is an expert on how to ask
questions and wrote

60
00:05:43,900 --> 00:05:48,400
[QUENTIN]
an entire book on this. And this should really come from us, the
type of

61
00:05:48,400 --> 00:05:53,240
[QUENTIN]
question we ask. But we definitely need to be able to filter out
and I

62
00:05:53,240 --> 00:05:58,400
[QUENTIN]
mean to be able to judge. So we do need our human intelligence to
understand

63
00:05:58,720 --> 00:06:02,680
[QUENTIN]
when there is something useful. When you have some Risks even if
we think about

64
00:06:02,680 --> 00:06:09,800
[QUENTIN]
this type of general intelligence. But back to basically this
view and often the discussion

65
00:06:09,800 --> 00:06:15,930
[QUENTIN]
mainly that I've seen recently by Sam Altman, for example, who
indeed seems that it's

66
00:06:15,930 --> 00:06:21,970
[QUENTIN]
kind of creating this almost godlike creator. And I think one
thing that I thought

67
00:06:21,970 --> 00:06:25,610
[QUENTIN]
also when you were speaking at the very beginning, I think a few
months after

68
00:06:25,610 --> 00:06:31,050
[QUENTIN]
ChatGPT 3.5 release or March 2023, there was this

69
00:06:33,690 --> 00:06:38,890
[QUENTIN]
document signed by thousands of experts on AI to say we should
stop the progress.

70
00:06:39,050 --> 00:06:44,050
[QUENTIN]
And I always felt that it, it sounds a bit like a marketing
campaign in

71
00:06:44,050 --> 00:06:48,130
[QUENTIN]
the sense it's so good what we are doing that we, you have to
stop

72
00:06:48,130 --> 00:06:51,890
[QUENTIN]
us. And I, I'm wondering if you, if you remember this and if what
you

73
00:06:51,890 --> 00:06:55,650
[QUENTIN]
thought about it and what you think today about this type of
action is it.

74
00:06:55,650 --> 00:06:59,410
[QUENTIN]
Is was it really genuine or was it more like a marketing trick?

75
00:07:00,050 --> 00:07:07,090
[DOROTHEA]
Honestly, I struggle to believe that anything or a lot that comes
from, you know,

76
00:07:07,170 --> 00:07:11,890
[DOROTHEA]
these circles is really genuine. It's. It's really hard to tell.
There are always like

77
00:07:12,160 --> 00:07:17,480
[DOROTHEA]
overlapping groups. And I think the sign for the moratorium, I
think it was this.

78
00:07:17,480 --> 00:07:23,440
[DOROTHEA]
There, there were people signing who are not aligned with Sam
Altman on other things.

79
00:07:23,440 --> 00:07:27,959
[DOROTHEA]
And so it's, it's really difficult to, to keep track of who is
involved where

80
00:07:27,959 --> 00:07:33,160
[DOROTHEA]
and who believes what. But I think in general hyping like, oh, we
will create

81
00:07:33,160 --> 00:07:38,400
[DOROTHEA]
super intelligence next month as well as saying like stop it,
it's so dangerous. Both

82
00:07:38,770 --> 00:07:43,650
[DOROTHEA]
is just feeding into the hype and it's not helpful. It's like
black or white.

83
00:07:44,130 --> 00:07:51,170
[DOROTHEA]
And this like the. If Sam Altman even, or guys know that are like
related

84
00:07:51,170 --> 00:07:55,290
[DOROTHEA]
to him or work on similar things, if they said, oh, we are so
good

85
00:07:55,290 --> 00:07:59,210
[DOROTHEA]
now, you need to stop us. This is just, it's dishonest because
they have kind

86
00:07:59,210 --> 00:08:04,450
[DOROTHEA]
of, they claim to have, you know, appropriated all the power to
create these things

87
00:08:04,850 --> 00:08:09,370
[DOROTHEA]
and they put themselves in a position of those who are the only
ones capable

88
00:08:09,370 --> 00:08:16,950
[DOROTHEA]
of keeping it under control. So it's like, you know, adopting a
really, an authority

89
00:08:16,950 --> 00:08:23,750
[DOROTHEA]
that no one has ever given them that's never been legitimized.
Insane.

90
00:08:24,070 --> 00:08:29,070
[QUENTIN]
Yeah, indeed. And the change, I mean, I'm pretty sure that's one
of the person

91
00:08:29,070 --> 00:08:32,950
[QUENTIN]
who signed this document was also Elon Musk, who is today trying
to lead on

92
00:08:32,950 --> 00:08:39,030
[QUENTIN]
the AI realm a few months, maybe one year later. So it's quite,
indeed surprising,

93
00:08:39,030 --> 00:08:42,060
[QUENTIN]
this kind of opposite view.

94
00:08:42,940 --> 00:08:48,060
[DOROTHEA]
Yeah. And even now. So Apple researchers from Apple have
published these, this paper, which

95
00:08:48,060 --> 00:08:51,580
[DOROTHEA]
is another on a side Note, it's interesting that most of the
research comes from

96
00:08:51,580 --> 00:08:56,580
[DOROTHEA]
companies now and there is hardly any independent university
research, which also, you know, must

97
00:08:56,580 --> 00:09:03,100
[DOROTHEA]
make us a bit wary of like, you know, the interests involved. And
so, so,

98
00:09:03,100 --> 00:09:07,420
[DOROTHEA]
so you could also say, oh, Apple, Apple is set to lag behind in
AI.

99
00:09:07,950 --> 00:09:11,430
[DOROTHEA]
Maybe it's in their interest to publish something that says,
okay, it's not going to

100
00:09:11,430 --> 00:09:15,790
[DOROTHEA]
work anyway, so. But I feel like a conspiracy theorist when I say
these things.

101
00:09:16,030 --> 00:09:20,670
[DOROTHEA]
I'm not, I'm not. But the fact is that anything that happens in
AI is

102
00:09:20,670 --> 00:09:25,550
[DOROTHEA]
so distinct from other fields of science, which are, as I said,
mostly, you know,

103
00:09:25,550 --> 00:09:32,830
[DOROTHEA]
cultivated by universities. And now we're having companies with
billions of money, billionaires like Musk

104
00:09:32,830 --> 00:09:39,540
[DOROTHEA]
and Bezos, et cetera, feeding this research, which is never free
from their own business

105
00:09:39,540 --> 00:09:43,580
[DOROTHEA]
interests. And that's also something we need to take into
account. So whose agenda is

106
00:09:43,580 --> 00:09:49,580
[DOROTHEA]
it? But again, I'm not talking about conspiracy theory. It's just
very different from, I

107
00:09:49,580 --> 00:09:53,660
[DOROTHEA]
don't know, research in chemistry or in physics as we know it.

108
00:09:55,820 --> 00:10:00,220
[QUENTIN]
We have seen during the inauguration of the second term of Donald
Trump, we have

109
00:10:00,220 --> 00:10:05,740
[QUENTIN]
this row of top tech leaders just behind him and it set the tone
and

110
00:10:06,040 --> 00:10:10,600
[QUENTIN]
on the role of tech leaders today and due to the sheer amount of
money

111
00:10:10,600 --> 00:10:14,920
[QUENTIN]
they have, but also the impact they might have on society. And
also here I'm

112
00:10:14,920 --> 00:10:21,320
[QUENTIN]
curious about your knowledge on ethical aspects of these
questions because it gives a lot

113
00:10:21,959 --> 00:10:28,440
[QUENTIN]
massive power to private companies to shift, for example, or
shape the public opinion from

114
00:10:28,600 --> 00:10:35,600
[QUENTIN]
managing the algorithm behind X, from Meta, but also on ChatGPT
because many people, hundreds

115
00:10:35,600 --> 00:10:40,960
[QUENTIN]
of millions are using it probably daily. I'm not entirely sure
about the numbers, but

116
00:10:41,120 --> 00:10:47,280
[QUENTIN]
a vast, basically set of people are using this daily. And you can
just shape

117
00:10:47,280 --> 00:10:53,680
[QUENTIN]
the way the tool will answer. You can also play with the training
sets and

118
00:10:54,240 --> 00:10:58,900
[QUENTIN]
erase some part of history if you would like to. You can also
push the

119
00:10:58,900 --> 00:11:03,460
[QUENTIN]
opinion in very pervasive way in some direction. And I'm just
curious if this is

120
00:11:03,460 --> 00:11:07,460
[QUENTIN]
also something you discuss or you see as a risk and how we should
deal

121
00:11:07,460 --> 00:11:13,820
[QUENTIN]
with the fact that private company have today such an important
impact on society.

122
00:11:15,020 --> 00:11:19,780
[DOROTHEA]
Yeah, it's certainly something to worry about because I mean,
ever since 1990s we've talked

123
00:11:19,780 --> 00:11:26,600
[DOROTHEA]
about corporations as political actors in business ethics. And
now, now no one, now it's

124
00:11:26,600 --> 00:11:31,160
[DOROTHEA]
not such a popular term anymore, but it seems to be just take for
granted.

125
00:11:31,400 --> 00:11:36,840
[DOROTHEA]
It's, it's become normal. And of course they have a lot of power.
You know,

126
00:11:37,400 --> 00:11:42,560
[DOROTHEA]
also the fact that they already have Billions and they're
spending billions directly into politics

127
00:11:42,560 --> 00:11:48,400
[DOROTHEA]
and into elections, etc. That's like their first influence. And
then of course with things

128
00:11:48,400 --> 00:11:56,010
[DOROTHEA]
like Starlink like Musk, he can switch off, you know, connection
wherever he wants. So

129
00:11:56,010 --> 00:12:01,210
[DOROTHEA]
these are the obvious things. But then again, I think it's like
of course they

130
00:12:01,210 --> 00:12:10,050
[DOROTHEA]
have most power, but everyone can use generative AI, especially
like large language models like

131
00:12:10,050 --> 00:12:15,090
[DOROTHEA]
ChatGPT in order to try to manipulate people. We've had this
scandal recently with this

132
00:12:15,090 --> 00:12:22,550
[DOROTHEA]
infamous study, unfortunately from University of Zurich, which is
nearby, where, you know, researchers tried

133
00:12:22,630 --> 00:12:30,390
[DOROTHEA]
to undercover feed ChatGPT into Reddit, was it, I think
discussion threads and try to

134
00:12:30,390 --> 00:12:35,910
[DOROTHEA]
convince people and change their opinion. So I think it's very
tempting for Anyone to

135
00:12:35,910 --> 00:12:41,070
[DOROTHEA]
use ChatGPT to their own benefit. Of course the ones who created
it, they have

136
00:12:41,070 --> 00:12:47,110
[DOROTHEA]
most power over it. But it's in general, it's a highly
manipulative tool in many

137
00:12:47,110 --> 00:12:50,710
[DOROTHEA]
ways. For better or worse, you know, you can say I have a good
agenda.

138
00:12:50,710 --> 00:12:55,190
[DOROTHEA]
It's okay, it's legitimate to try to convince people to be less
racist or whatever,

139
00:12:55,190 --> 00:12:59,350
[DOROTHEA]
but it's a fact that everyone can use it however they like.

140
00:12:59,830 --> 00:13:03,950
[QUENTIN]
Yeah, so that was exactly my reaction, I think actually in the
first podcast where

141
00:13:03,950 --> 00:13:08,630
[QUENTIN]
we, I started with this paper I, I would say published in
Science, which showed

142
00:13:08,630 --> 00:13:16,830
[QUENTIN]
that you could change the belief in conspiracy stories of people
on long term. It

143
00:13:16,830 --> 00:13:21,940
[QUENTIN]
was really long, lasting at least a few months. What was meant by
long here

144
00:13:22,420 --> 00:13:28,100
[QUENTIN]
and by discussing with these chatbots that were tuned to
basically help to shift the

145
00:13:28,100 --> 00:13:32,420
[QUENTIN]
belief of people. And my first reaction was like, you could do
the opposite, right?

146
00:13:32,500 --> 00:13:39,940
[QUENTIN]
Because it's very convincing, it's always available, it's
infinitely patient and it has this inherent

147
00:13:39,940 --> 00:13:46,500
[QUENTIN]
way of basically being very convincing. And also because it has
been trained on correct

148
00:13:46,500 --> 00:13:51,060
[QUENTIN]
answers, basically it was fed with this is a question, this is
the correct answer.

149
00:13:51,360 --> 00:13:56,200
[QUENTIN]
So then basically these tools learned extensively to be
convincing and to pretend that they

150
00:13:56,200 --> 00:14:03,840
[QUENTIN]
know. Not only because humans often are over optimistic, over
enthusiastic, heroic with their knowledge,

151
00:14:04,160 --> 00:14:07,840
[QUENTIN]
but also because specifically in the training set and the way it
was trained, it

152
00:14:07,840 --> 00:14:12,720
[QUENTIN]
was fed with a lot of correct answers by experts. So if you just
show

153
00:14:12,720 --> 00:14:17,040
[QUENTIN]
this and as these tools are mimicking, and when I say this, it's
really LLMs

154
00:14:17,520 --> 00:14:24,280
[QUENTIN]
are mimicking our behavior. They mostly observed people who were
confident of their answers. So

155
00:14:24,280 --> 00:14:28,080
[QUENTIN]
then basically you have a lot of ways to reduce these issues and
they are

156
00:14:28,080 --> 00:14:31,080
[QUENTIN]
work on this and they are trying to do this also with web search
and

157
00:14:31,080 --> 00:14:37,640
[QUENTIN]
other techniques. But, but it tends to, to be very convincing.
And you see that

158
00:14:37,640 --> 00:14:42,720
[QUENTIN]
indeed many people who are now exposed to this just take it for
granted because

159
00:14:42,880 --> 00:14:43,320
[QUENTIN]
I know.

160
00:14:43,320 --> 00:14:48,160
[DOROTHEA]
But some people think they're like the benevolent dictators that
never existed and they are

161
00:14:48,160 --> 00:14:53,140
[DOROTHEA]
allowed because for good. And that's. It's just plain wrong. But
also what's striking, as

162
00:14:53,140 --> 00:14:59,060
[DOROTHEA]
you say, I don't know. That's not something that he has ever
learned. And it

163
00:14:59,060 --> 00:15:03,300
[DOROTHEA]
never, for example, has like the three dots for like thinking,
you know, it pretends

164
00:15:03,300 --> 00:15:08,780
[DOROTHEA]
to think. It's like I'm thinking now, reasoning, but it's really.
And it's, it's annoying

165
00:15:08,780 --> 00:15:14,180
[DOROTHEA]
because it's also been trained to be very, how they say,
sycophantic, like people pleasing,

166
00:15:14,180 --> 00:15:18,790
[DOROTHEA]
et cetera. But that's why we in some situations prefer it over
people because other

167
00:15:18,790 --> 00:15:22,630
[DOROTHEA]
people don't please us all the time. So. But these are
psychological issues. But you

168
00:15:22,630 --> 00:15:28,430
[DOROTHEA]
know, what I find particularly striking about the fact that
these, like AI companies have

169
00:15:28,430 --> 00:15:34,030
[DOROTHEA]
so much power is that at the same time they claim to be
democratizing something.

170
00:15:34,190 --> 00:15:39,590
[DOROTHEA]
They say we're democratizing AI, et cetera. They are playing with
the word democracy when

171
00:15:39,590 --> 00:15:43,310
[DOROTHEA]
they in fact are undermining democracy in so many ways.

172
00:15:45,230 --> 00:15:48,830
[QUENTIN]
Will you say are the main ways they affect democracy?

173
00:15:49,550 --> 00:15:54,350
[DOROTHEA]
Well, as we said, you know, be it by directly, you know, spending
money on

174
00:15:54,350 --> 00:16:04,030
[DOROTHEA]
elections, like by trading government agencies that have never
been really legitimized by, you know,

175
00:16:04,190 --> 00:16:09,110
[DOROTHEA]
training the algorithms and, you know, deciding who gets to
publish what on their social

176
00:16:09,110 --> 00:16:17,390
[DOROTHEA]
media platforms by just making these tools available that, as we
just discussed, give anyone,

177
00:16:18,270 --> 00:16:27,030
[DOROTHEA]
you know, huge opportunities to do their, to influence others,
etc. So in many, many

178
00:16:27,030 --> 00:16:33,870
[DOROTHEA]
ways. But, yeah, so. But at the same time, we're democratizing
knowledge, we are democratizing

179
00:16:33,870 --> 00:16:40,760
[DOROTHEA]
creativity, we are democratizing art, we're democratizing
education. What do you mean? You're just

180
00:16:43,080 --> 00:16:50,120
[DOROTHEA]
commodifying things on a large scale. You're, you know, selling
your products. You're just talking

181
00:16:50,120 --> 00:16:55,240
[DOROTHEA]
about access in terms of quantity and you're not thinking ahead
of what it really

182
00:16:55,240 --> 00:17:02,320
[DOROTHEA]
means and on whose back. You're basically building up this axis
for others. So, yeah,

183
00:17:02,320 --> 00:17:08,139
[DOROTHEA]
that's one of my pet peeves with the whole, you know, training
history of all

184
00:17:08,139 --> 00:17:13,139
[DOROTHEA]
these models. Like, what were they trained on? Where did they
take that stuff from?

185
00:17:13,139 --> 00:17:17,899
[DOROTHEA]
Who got compensated? Wait, no one. And so why do you call
democratization of art

186
00:17:17,899 --> 00:17:20,419
[DOROTHEA]
now? When she stole it?

187
00:17:20,419 --> 00:17:26,379
[QUENTIN]
Basically, yeah, this is you. No, no, but this is a very
important one. And

188
00:17:26,379 --> 00:17:31,059
[QUENTIN]
there was this very good framing in a TED talk recently, but by
Ed Newton

189
00:17:31,059 --> 00:17:33,699
[QUENTIN]
Rex on copyright Basically, and.

190
00:17:33,699 --> 00:17:35,139
[DOROTHEA]
He says, one of my favorites.

191
00:17:35,379 --> 00:17:36,419
[QUENTIN]
Yeah, it's very.

192
00:17:36,499 --> 00:17:40,290
[DOROTHEA]
I haven't seen the TED Talk, but I really appreciate. Yeah.

193
00:17:40,290 --> 00:17:44,770
[QUENTIN]
And the, basically the. So for the audience, the, basically the
pitch in the beginning

194
00:17:44,770 --> 00:17:55,409
[QUENTIN]
of this, of this TED was these big AI companies, they have, they
require brains

195
00:17:55,409 --> 00:18:02,850
[QUENTIN]
or engineers, a lot of data and a compute and they are ready to
spend

196
00:18:02,850 --> 00:18:10,680
[QUENTIN]
hundreds of millions under engineers, sometimes billions on the
compute. So the computers. And for

197
00:18:10,680 --> 00:18:16,160
[QUENTIN]
some reason for the third data, they expected to get it for free.
And you

198
00:18:16,160 --> 00:18:20,680
[QUENTIN]
have this. Recently they said, I can't remember who said that,
but basically that they

199
00:18:20,680 --> 00:18:27,200
[QUENTIN]
used hundreds, thousands of books under copyright because they
had no economic values. But if

200
00:18:27,200 --> 00:18:28,960
[QUENTIN]
you used it because it.

201
00:18:28,960 --> 00:18:35,540
[DOROTHEA]
Has a value for you, it's so cynical. It's so cynical. So Nick
Clegg, the

202
00:18:35,780 --> 00:18:43,340
[DOROTHEA]
former British politician, then gone Meta executive, he recently
said, oh, if we had to

203
00:18:43,340 --> 00:18:49,380
[DOROTHEA]
compensate all the authors for the work that we use to train our
AI models,

204
00:18:49,380 --> 00:18:54,740
[DOROTHEA]
then the AI industry would go bankrupt. So I mean, I mean, how
dare you.

205
00:18:54,740 --> 00:19:00,310
[DOROTHEA]
So does this legitimize it? And, and it's just, it's. I mean
really, I. That

206
00:19:00,390 --> 00:19:05,350
[DOROTHEA]
Meta used this Russian pirate database, Libchen called

207
00:19:07,750 --> 00:19:13,310
[DOROTHEA]
all, you know, millions and millions of books and texts to train
their llama. Their

208
00:19:13,310 --> 00:19:17,750
[DOROTHEA]
model. And there was a piece in the Atlantic where you could
check whether on

209
00:19:17,750 --> 00:19:22,550
[DOROTHEA]
that paratized database, you know, what works were in there. So I
entered my name

210
00:19:22,550 --> 00:19:27,070
[DOROTHEA]
and I realized my thesis, every single article that I've ever
published is in that

211
00:19:27,070 --> 00:19:33,410
[DOROTHEA]
database. So maybe I should feel flattered that maybe Meta used
my work. So finally

212
00:19:33,410 --> 00:19:39,530
[DOROTHEA]
someone read it their LLM to feed their. Their AI. But it's just,
it's outrageous

213
00:19:39,530 --> 00:19:44,890
[DOROTHEA]
what lengths they're willing to go to in order to, you know,
expand their power.

214
00:19:46,650 --> 00:19:47,370
[DOROTHEA]
Unbelievable.

215
00:19:47,450 --> 00:19:53,170
[QUENTIN]
It makes sense. For me at least now it feels that with everything
we are

216
00:19:53,170 --> 00:19:58,970
[QUENTIN]
saying, I might understand why they want to pretend it's like
they are creating some

217
00:19:59,210 --> 00:20:04,230
[QUENTIN]
superhuman intelligence. Because if it's for the greater good, if
it's for this big mission,

218
00:20:04,230 --> 00:20:05,150
[QUENTIN]
you basically, it's.

219
00:20:09,070 --> 00:20:15,350
[DOROTHEA]
A greater good to democratizing, universalizing, whatever agenda.
And then this is. Okay. And this

220
00:20:15,350 --> 00:20:18,430
[DOROTHEA]
is just pure rhetoric. It's really, it's totally dishonest. Yeah.

221
00:20:18,990 --> 00:20:22,670
[QUENTIN]
So it could be very provocative. It could be like some people who
use religion

222
00:20:22,670 --> 00:20:27,550
[QUENTIN]
for this. Like, it's not me, it's God and it allows me to do
whatever

223
00:20:27,870 --> 00:20:32,500
[QUENTIN]
and it's for the greater good, for humanity. And yeah, it's.

224
00:20:32,500 --> 00:20:38,220
[DOROTHEA]
But it's also, you know, as we know, they don't even make money
off it

225
00:20:38,220 --> 00:20:41,860
[DOROTHEA]
now of their LLMs, it's really hard to make money with them and
doesn't pay

226
00:20:41,860 --> 00:20:45,820
[DOROTHEA]
off now. So a lot of that talk is also actually directed at
investors to

227
00:20:45,820 --> 00:20:51,340
[DOROTHEA]
keep investors interested. So that's also one of the problems
when I said like research

228
00:20:51,340 --> 00:20:55,500
[DOROTHEA]
is being done by the companies themselves. You know, a lot of the
research that's

229
00:20:55,500 --> 00:21:01,220
[DOROTHEA]
published is trying to please investors. And you need to talk up
AI and make

230
00:21:01,220 --> 00:21:06,940
[DOROTHEA]
AI sound better than it is. And at the same time what they also
do

231
00:21:06,940 --> 00:21:12,220
[DOROTHEA]
is they talk down humans, they reduce us to computers. So they
say, oh, the

232
00:21:12,220 --> 00:21:16,460
[DOROTHEA]
human brain is only just a computer anyway. We can easily copy it
so we

233
00:21:16,460 --> 00:21:21,860
[DOROTHEA]
creating something better. So I think it's both ways. Talking up
the skills of AI

234
00:21:22,020 --> 00:21:28,030
[DOROTHEA]
and playing down the skills or the complexity of humans. And it
goes together to,

235
00:21:28,110 --> 00:21:33,230
[DOROTHEA]
to create this agenda that makes people think like oh, they are
our saviors, they're

236
00:21:33,230 --> 00:21:34,430
[DOROTHEA]
working for a greater good.

237
00:21:35,630 --> 00:21:40,150
[QUENTIN]
And even the terminology in AI might be problematic with that
respect because the fact

238
00:21:40,150 --> 00:21:46,230
[QUENTIN]
that we use neurons. Exactly. Because it has in some way
initially the. The concept

239
00:21:46,230 --> 00:21:51,380
[QUENTIN]
of mimicking the way our brain works. But we know that it's
widely different, it's

240
00:21:51,380 --> 00:21:55,460
[QUENTIN]
extensively different. And all the terminology, I mean if you are
in statistics, it's not

241
00:21:55,460 --> 00:22:00,060
[QUENTIN]
really exciting the terminology. It's really terms that are.
There is nothing fancy about this

242
00:22:00,060 --> 00:22:10,740
[QUENTIN]
terminology, whatever. And in artificial intelligence it tends to
have really exciting terminology like already

243
00:22:10,740 --> 00:22:19,750
[QUENTIN]
artificial intelligence neurons, deep neural networks and all
these things might also hides or makes

244
00:22:19,750 --> 00:22:26,070
[QUENTIN]
us fantasize about what, what's behind this, this math. Because
these are just matrix, completion,

245
00:22:27,110 --> 00:22:32,670
[QUENTIN]
multiplication. It's, it's really. But it's, it's, it's. Anyway,
even if you can reduce it

246
00:22:32,670 --> 00:22:37,710
[QUENTIN]
to some mathematical operation, it's still very impressive what
it's capable of today. Even if

247
00:22:37,710 --> 00:22:45,590
[QUENTIN]
it has cost and implication and issues. You also have some, some
very surprising effect

248
00:22:45,590 --> 00:22:53,910
[QUENTIN]
and indeed I mean also related to ethical issues. I always seen
this situation as

249
00:22:54,150 --> 00:22:59,470
[QUENTIN]
a world's natural experiment. It's really like we are not sure
what these tools are

250
00:22:59,470 --> 00:23:04,110
[QUENTIN]
capable of, but play with them and here they are and we'll see
where it

251
00:23:04,110 --> 00:23:07,950
[QUENTIN]
goes. And I feel that it's also. I don't know how it's viewed
from an

252
00:23:07,950 --> 00:23:13,810
[QUENTIN]
ethical perspective from someone expert in this, but it feels
like there's, there's clearly some.

253
00:23:13,890 --> 00:23:16,810
[QUENTIN]
It's a recipe for some dangers.

254
00:23:16,810 --> 00:23:22,570
[DOROTHEA]
Let's say it is I think also what makes. I mean, and I must also

255
00:23:22,570 --> 00:23:28,250
[DOROTHEA]
emphasize I appreciate AI. I appreciate, you know, it in all
kinds of forms. The

256
00:23:28,250 --> 00:23:32,690
[DOROTHEA]
ones I Notice and actively use and the other ones that kind of
guide me,

257
00:23:33,490 --> 00:23:39,080
[DOROTHEA]
you know, without me knowing it. I mean, I'm totally not anti AI
and you

258
00:23:39,080 --> 00:23:42,840
[DOROTHEA]
know, really think there's a lot of good to be gained from it.
But as

259
00:23:42,840 --> 00:23:47,920
[DOROTHEA]
you say, one of the problems in my eyes is also that we are
always

260
00:23:47,920 --> 00:23:54,200
[DOROTHEA]
talking about general purpose AI. So that already says like,
okay, we created something that

261
00:23:54,200 --> 00:23:57,600
[DOROTHEA]
channel purpose, so up to you what you use it for. So we are not

262
00:23:57,600 --> 00:24:02,200
[DOROTHEA]
responsible for it. It's general purpose. You know, it's like
whether you use a hammer

263
00:24:02,200 --> 00:24:06,400
[DOROTHEA]
to hit the nail or to kill someone, it's your responsibility. But
this channel purpose

264
00:24:07,350 --> 00:24:15,670
[DOROTHEA]
also, like framework and, and rhetoric is really just escaping
accountability or like passing on

265
00:24:16,230 --> 00:24:21,990
[DOROTHEA]
responsibility to those who use it. But imagine if maybe it's a
stupid comparison, but

266
00:24:21,990 --> 00:24:28,270
[DOROTHEA]
if nuclear technology was general purpose and anyone could just
use it, you know, you

267
00:24:28,270 --> 00:24:34,310
[DOROTHEA]
wouldn't. And of course I'm not comparing the direct dangers of
generative AI with nuclear

268
00:24:34,310 --> 00:24:41,450
[DOROTHEA]
technology, but it's just like this general purpose mindset that
you're releasing something that's so

269
00:24:41,450 --> 00:24:48,050
[DOROTHEA]
powerful under the label or with the frame general purpose should
not be ignored.

270
00:24:49,570 --> 00:24:54,010
[QUENTIN]
So this leads also to this basically confusion that we have
today. So I was

271
00:24:54,010 --> 00:24:59,290
[QUENTIN]
giving a talk recently to C Suites about AI and different type of
AI and

272
00:24:59,290 --> 00:25:03,930
[QUENTIN]
I realized that for some people, even if I did my best to explain
what

273
00:25:03,930 --> 00:25:08,660
[QUENTIN]
I was mentioning by AI, for some at the very end they thought
where can

274
00:25:08,660 --> 00:25:14,220
[QUENTIN]
I use this predictive AI? Basically so things that we have been
doing for long

275
00:25:14,220 --> 00:25:19,900
[QUENTIN]
time, decades in companies that these issues are working, they
all pictured basically AI as

276
00:25:19,980 --> 00:25:25,020
[QUENTIN]
some kind of chatgpt with different functionalities. But it is
one specific type, right? It's

277
00:25:25,020 --> 00:25:30,500
[QUENTIN]
really generative AI and large language models in particular. So
you mentioned this also, if

278
00:25:30,500 --> 00:25:35,910
[QUENTIN]
I'm not mistaken several times, that this, that this also creates
issues when we are

279
00:25:35,910 --> 00:25:41,270
[QUENTIN]
not clear about what we are discussing. Is it AI for prediction,
for forecasting in

280
00:25:41,270 --> 00:25:48,270
[QUENTIN]
financial sector, for example, to detect and anticipate fraud, or
is it Genai, which is

281
00:25:48,270 --> 00:25:48,910
[QUENTIN]
very different?

282
00:25:51,230 --> 00:25:58,470
[DOROTHEA]
Yeah, it's in many ways it's really important that we separate
these because on the

283
00:25:58,470 --> 00:26:04,390
[DOROTHEA]
one hand, just be aware that AI is more than just generative AI
and that

284
00:26:04,790 --> 00:26:10,270
[DOROTHEA]
the ethical issues that are related to generative AI are
different from the ones like

285
00:26:10,270 --> 00:26:17,630
[DOROTHEA]
risks and opportunities from the ones in predictive AI. But it's
this generative AI discourse

286
00:26:17,630 --> 00:26:21,590
[DOROTHEA]
is just dominating everything. And I've also seen a CEO of one of
the big

287
00:26:21,590 --> 00:26:26,550
[DOROTHEA]
four consulting companies in, in Switzerland, when he was asked
on a Panel like, so

288
00:26:26,790 --> 00:26:31,540
[DOROTHEA]
how does AI play into your, you know, role as a CEO of this big

289
00:26:31,540 --> 00:26:37,180
[DOROTHEA]
company? He said, oh, I, I have an LLM that helps me write emails
and

290
00:26:37,180 --> 00:26:40,820
[DOROTHEA]
like, if this is like one of, you know, the big four, if this is

291
00:26:40,820 --> 00:26:43,860
[DOROTHEA]
what comes into the mind of one of the CEOs, one of big four,
it's,

292
00:26:43,860 --> 00:26:49,260
[DOROTHEA]
it's really a bit worrying. But I think also. So on the one hand
generative

293
00:26:49,260 --> 00:26:54,060
[DOROTHEA]
AI has its problems as we have discussed its general purpose.
It's, it's, you know,

294
00:26:54,380 --> 00:27:01,680
[DOROTHEA]
manipulative potential manipulative, potentially biased, can
also, you know, create access for people to tools

295
00:27:01,680 --> 00:27:06,040
[DOROTHEA]
that they didn't have, etc. For the, you know, on a positive
note. But predictive

296
00:27:06,200 --> 00:27:12,880
[DOROTHEA]
or AI for example, is single purpose. Often that's used for a
specific task. For

297
00:27:12,880 --> 00:27:18,440
[DOROTHEA]
example, when it comes to climate science, you're predicting
weather events or analyzing, you know,

298
00:27:18,440 --> 00:27:23,800
[DOROTHEA]
biodiversity, whatever, which is very helpful and which is hands
on and which is less

299
00:27:23,800 --> 00:27:31,620
[DOROTHEA]
energy intense and which in the case of biodiversity doesn't hurt
anyone's privacy or data

300
00:27:31,780 --> 00:27:37,700
[DOROTHEA]
hasn't been, you know, stolen mostly I hope. And so we're missing
all the opportunities.

301
00:27:37,860 --> 00:27:42,500
[DOROTHEA]
So also for those who are like the doomsday profits when it comes
to AI,

302
00:27:42,580 --> 00:27:49,540
[DOROTHEA]
they only focus on, on generative or like AGI promises, etc. But
there is a

303
00:27:49,540 --> 00:27:55,260
[DOROTHEA]
lot of power like positive potential in AI, but maybe more so in
single purpose

304
00:27:55,260 --> 00:28:01,060
[DOROTHEA]
AI where we specifically, you know, develop and use an AI to
application to, to

305
00:28:01,060 --> 00:28:06,580
[DOROTHEA]
address specific problem in just of just instead of just, you
know, giving away the

306
00:28:06,580 --> 00:28:12,700
[DOROTHEA]
general purpose AI and then see what happens. So I think in many
ways it's,

307
00:28:12,700 --> 00:28:19,380
[DOROTHEA]
it's, it's, it's not good that this, this lopsided focus on
generative AI and also

308
00:28:19,380 --> 00:28:22,940
[DOROTHEA]
I appreciate a lot. One of the best books I've read recently is
AI Snake

309
00:28:23,320 --> 00:28:29,640
[DOROTHEA]
from Arvind Narayanan and Kapoor where they show so many use
cases. Of course they

310
00:28:29,640 --> 00:28:33,760
[DOROTHEA]
look at them from a critical perspective. But that's also
important on, you know, what

311
00:28:33,760 --> 00:28:38,720
[DOROTHEA]
can you really predict from data? How can human behavior be
predicted? What is possible,

312
00:28:38,720 --> 00:28:43,480
[DOROTHEA]
what is not possible? Where can we rely on, you know, machine
learning or AI

313
00:28:43,880 --> 00:28:50,360
[DOROTHEA]
to help us and where are the boundaries and this. All these
aspects get bypassed

314
00:28:50,360 --> 00:28:52,800
[DOROTHEA]
by everyone talking about their latest prompts.

315
00:28:54,950 --> 00:28:59,510
[QUENTIN]
It's true that while you are saying this, I could have this image
of how

316
00:28:59,590 --> 00:29:06,190
[QUENTIN]
everybody was shocked and how it was relayed and everybody tried
and also because it

317
00:29:06,190 --> 00:29:10,550
[QUENTIN]
was accessible, I guess. But with ChatGPT 3.5 everybody was
amazed that it could write

318
00:29:10,870 --> 00:29:18,590
[QUENTIN]
Some kind of poems. For example, quickly, while you had AI before
helping to save

319
00:29:18,590 --> 00:29:22,710
[QUENTIN]
lives, for example, by helping to detect tumor with MRI scan.

320
00:29:22,710 --> 00:29:23,310
[DOROTHEA]
Exactly.

321
00:29:23,460 --> 00:29:29,060
[QUENTIN]
And this is like not so, so popular or so well known by many
people.

322
00:29:29,060 --> 00:29:29,460
[QUENTIN]
So.

323
00:29:29,780 --> 00:29:35,460
[DOROTHEA]
But even then, for example, when it comes to, you know, image
recognition, radiology, etc.

324
00:29:36,820 --> 00:29:41,260
[DOROTHEA]
It's so important to use AI in that context. But then you have
the people

325
00:29:41,260 --> 00:29:47,260
[DOROTHEA]
who immediately claim we will not need radiologists anymore. They
immediately impose themselves on people

326
00:29:47,260 --> 00:29:52,130
[DOROTHEA]
and. Or like, yeah. Then they claim they want to do away with the
humans,

327
00:29:52,130 --> 00:29:57,290
[DOROTHEA]
which then triggers all the critics who focus on the cases where
AI has not

328
00:29:57,290 --> 00:30:02,530
[DOROTHEA]
recognized the tumor instead of combining the two, you know,
okay, so we have AI

329
00:30:02,530 --> 00:30:07,330
[DOROTHEA]
that can help us with image recognition in medical settings, but
we still need humans.

330
00:30:07,330 --> 00:30:11,650
[DOROTHEA]
That's clear. Because all the benchmarks that they create to
measure how well an AI

331
00:30:12,290 --> 00:30:18,290
[DOROTHEA]
performs in radiology are totally taken out of context. They are
not applicable to the

332
00:30:18,290 --> 00:30:23,570
[DOROTHEA]
everyday work of radiologists anyway. But it's, it's like such a
talk, sorry, toxic culture.

333
00:30:23,570 --> 00:30:27,650
[DOROTHEA]
So the ones say, oh, we have AI now, go away radiologists, we
don't need

334
00:30:27,650 --> 00:30:32,450
[DOROTHEA]
you anymore. And the others who say, like, it should never be
used, it must

335
00:30:32,450 --> 00:30:37,450
[DOROTHEA]
never be used, etc. It's too dangerous. And why can't you see it
as a

336
00:30:37,450 --> 00:30:41,450
[DOROTHEA]
complementary relation? So it's really striking.

337
00:30:41,770 --> 00:30:46,490
[QUENTIN]
Yeah. They are rarely the case that. So in some situations,
automation most of the

338
00:30:46,490 --> 00:30:53,260
[QUENTIN]
time might be augmentation. Indeed. I mean, that's one of the,
the, the type of

339
00:30:53,260 --> 00:30:57,900
[QUENTIN]
post I see the most on LinkedIn that is driving me crazy is reap.
And

340
00:30:58,220 --> 00:31:02,740
[QUENTIN]
you say some kind of, of work, like rest in peace. Legal.

341
00:31:02,740 --> 00:31:03,740
[DOROTHEA]
Yeah. Oh, okay.

342
00:31:03,740 --> 00:31:04,380
[QUENTIN]
Developers.

343
00:31:04,460 --> 00:31:04,940
[DOROTHEA]
Yes.

344
00:31:05,020 --> 00:31:09,900
[QUENTIN]
And you're like, well, you know, so far it's not what we are
seeing. It's

345
00:31:09,900 --> 00:31:14,420
[QUENTIN]
helping to augment if you do it well. But sure, there are some
job destruction.

346
00:31:14,420 --> 00:31:15,020
[QUENTIN]
I mean, you.

347
00:31:15,340 --> 00:31:21,740
[DOROTHEA]
Yeah, sure, but that's what I'm saying. Why do they feel so
obsessed with playing

348
00:31:21,820 --> 00:31:26,060
[DOROTHEA]
up or talking up the skills of AI and playing down the skills of
humans?

349
00:31:26,060 --> 00:31:30,140
[DOROTHEA]
Why, why is it, why can't you just say, hey, we have a helpful
tool

350
00:31:30,140 --> 00:31:34,500
[DOROTHEA]
here, humans, you can do your jobs better. And of course some
jobs will, you

351
00:31:34,500 --> 00:31:40,260
[DOROTHEA]
know, be become obsolete as always in history. But there's like
either the machine or

352
00:31:40,260 --> 00:31:43,020
[DOROTHEA]
the human. It's just not helpful.

353
00:31:43,900 --> 00:31:50,300
[QUENTIN]
But interestingly, I see some people who start to realize the
value of humans due

354
00:31:50,300 --> 00:31:54,940
[QUENTIN]
to this substitution that some people are pushing and they're
like, actually, I really like

355
00:31:54,940 --> 00:31:59,020
[QUENTIN]
to exchange with a human to learn from A new human to be in a

356
00:31:59,020 --> 00:32:07,460
[QUENTIN]
room with a physical person with their limitations, physical
experience of the world and emotions

357
00:32:07,620 --> 00:32:12,940
[QUENTIN]
and suddenly we realize that this or we some people, it helps
them to realize

358
00:32:12,940 --> 00:32:17,770
[QUENTIN]
that actually the all the uniqueness and valuable things that
humans have to offer. But

359
00:32:17,770 --> 00:32:21,890
[QUENTIN]
indeed on the other hand of the spectrum, I guess you have other
people who

360
00:32:22,050 --> 00:32:28,130
[QUENTIN]
for some reason would like to automate, replace everything and
does this doesn't seem to

361
00:32:28,130 --> 00:32:32,610
[QUENTIN]
have any answer. What, what will be then the place of humans in
this society.

362
00:32:33,410 --> 00:32:34,890
[QUENTIN]
Which is, which is peculiar.

363
00:32:34,890 --> 00:32:36,210
[DOROTHEA]
Yeah. Yes.

364
00:32:36,530 --> 00:32:43,690
[QUENTIN]
So when, when you go and talk to leaders in the business world,
how do

365
00:32:43,690 --> 00:32:48,290
[QUENTIN]
you see that this hype. How do you deal with this hype and
basically help

366
00:32:48,290 --> 00:32:53,530
[QUENTIN]
them to basically see it where, how they should see it or what
are the

367
00:32:53,530 --> 00:32:58,930
[QUENTIN]
implication you. You see from this hyping of AI in the business
world with leaders?

368
00:33:00,050 --> 00:33:06,450
[DOROTHEA]
Well, I mean I'm mostly you know, teaching in continuing
education like for example AI

369
00:33:06,450 --> 00:33:12,970
[DOROTHEA]
and ethics in a, in a class on AI and finance. So there for
example

370
00:33:13,210 --> 00:33:20,650
[DOROTHEA]
I developed a credit worthiness game with algorithms and, and I
played with different notions

371
00:33:20,650 --> 00:33:24,690
[DOROTHEA]
of fairness just to show them the implications. If you think like
you have an

372
00:33:24,690 --> 00:33:29,930
[DOROTHEA]
algorithm that's deciding who gets a credit and who does not. And
then I kind

373
00:33:29,930 --> 00:33:35,610
[DOROTHEA]
of play around with like the, the normative or like. Yeah,
normative like moral assumptions

374
00:33:35,610 --> 00:33:39,410
[DOROTHEA]
that are underlying different conceptions of fairness. So I
always keep it on a very

375
00:33:39,410 --> 00:33:44,880
[DOROTHEA]
appl in my teaching and in my keynotes. Yeah, it's mostly kind of
tongue in

376
00:33:44,880 --> 00:33:49,440
[DOROTHEA]
cheek also like showing that this is what's possible and wait
here there's something here.

377
00:33:49,600 --> 00:33:55,280
[DOROTHEA]
I'm always ending with a strong plea for the human element must
be kept in

378
00:33:55,280 --> 00:34:02,120
[DOROTHEA]
loop. But of course, I mean with people who are like totally
determined to hype

379
00:34:02,120 --> 00:34:07,920
[DOROTHEA]
something or who have also as I say skin skin in the game, like
you

380
00:34:07,920 --> 00:34:11,980
[DOROTHEA]
know, why would I want to ruin their dreams? But they don't get
in touch

381
00:34:11,980 --> 00:34:16,980
[DOROTHEA]
with me anyway. So that's you know, not meeting those too often.
But yeah, in

382
00:34:16,980 --> 00:34:21,900
[DOROTHEA]
continuing education you have people from different companies and
you can see, you know, different

383
00:34:21,900 --> 00:34:28,020
[DOROTHEA]
levels of literacy. And I just always work with cases and case
studies and games

384
00:34:28,020 --> 00:34:37,140
[DOROTHEA]
etc. To kind of reduce the distance between them and the
algorithm to see, you

385
00:34:37,140 --> 00:34:40,940
[DOROTHEA]
know, what happens when you use AI in such and such setting.

386
00:34:41,660 --> 00:34:47,220
[QUENTIN]
I see, I see. Yeah, interesting. And then indeed I've read some
research also to

387
00:34:47,220 --> 00:34:52,540
[QUENTIN]
try to see when you don't you let just these also Gen AI tool
play

388
00:34:52,700 --> 00:34:57,820
[QUENTIN]
traditional games we used in sociology and economics to see what
are Their traits, basically

389
00:34:57,820 --> 00:35:02,820
[QUENTIN]
their behavior. And it was rather interesting to see their
standard answer and what it

390
00:35:02,820 --> 00:35:07,300
[QUENTIN]
has learned basically. And of course you can tweak it, but it's
interesting to see

391
00:35:07,300 --> 00:35:13,570
[QUENTIN]
already the same standard answer. And based as you discussed
about teaching, that's also a

392
00:35:13,570 --> 00:35:19,290
[QUENTIN]
huge topic. I mean I have also many students every year and young
professionals and,

393
00:35:19,850 --> 00:35:24,890
[QUENTIN]
and I mean it's, it has serious impact on the way people learn.
But also

394
00:35:25,130 --> 00:35:31,490
[QUENTIN]
mainly something I wanted to discuss with you about this
basically democratization, inequalities and the

395
00:35:31,490 --> 00:35:36,410
[QUENTIN]
fact that you, you have some research showing well in these
developing countries where it's

396
00:35:36,410 --> 00:35:42,370
[QUENTIN]
hard to have the resources, we can automatize the teacher or
augment the teacher or

397
00:35:42,370 --> 00:35:48,650
[QUENTIN]
replace the teacher entirely with an AI tool. It might help to
give access to

398
00:35:48,650 --> 00:35:53,330
[QUENTIN]
education to many people. And in this discussion as well as in
other topics like

399
00:35:53,330 --> 00:35:58,650
[QUENTIN]
mental health, I'm always. I see the benefits because I see some
research that I

400
00:35:58,650 --> 00:36:04,930
[QUENTIN]
find convincing where I see that it can help if it's well made
and well

401
00:36:05,170 --> 00:36:10,230
[QUENTIN]
done. But in other situation I'm a bit also afraid that then we
might say

402
00:36:10,630 --> 00:36:16,230
[QUENTIN]
to the poorer you'll get the AI and to the richer you'll get the
real

403
00:36:16,230 --> 00:36:19,830
[QUENTIN]
experts who can afford a human. Exactly.

404
00:36:20,310 --> 00:36:24,350
[DOROTHEA]
Same doctors. You know, if you don't have a doctor in your
village and you

405
00:36:24,350 --> 00:36:30,350
[DOROTHEA]
have AI based medicine, of course it's like better than nothing.
But. Yeah, yeah. But

406
00:36:30,350 --> 00:36:35,210
[DOROTHEA]
I must say I'm careful with the whole education context because
I'm only a guest

407
00:36:35,210 --> 00:36:40,610
[DOROTHEA]
lecturer. Luckily at universities I'm not involved in exams so I
can't really say anything

408
00:36:40,610 --> 00:36:43,530
[DOROTHEA]
or I'm lucky not to have to deal with the fact how do we make

409
00:36:43,530 --> 00:36:49,930
[DOROTHEA]
meaningful exams when everyone is using ChatGPT, etc. And, and,
and the psychological questions about

410
00:36:49,930 --> 00:36:57,610
[DOROTHEA]
mental health is also something that yeah, I'm not really
qualified to say but as

411
00:36:57,610 --> 00:37:01,570
[DOROTHEA]
you say like the value of the human and as you said before, it's,
it's

412
00:37:01,570 --> 00:37:08,950
[DOROTHEA]
a paradoxical effect that the more we get to see AI, maybe the
more distinctive

413
00:37:08,950 --> 00:37:14,750
[DOROTHEA]
is the value of the human interaction that we still have. But as
to for

414
00:37:14,750 --> 00:37:23,190
[DOROTHEA]
example democratizing access to education, I mean the first thing
is you have public libraries,

415
00:37:24,230 --> 00:37:28,470
[DOROTHEA]
create public libraries. You don't need an AI generated teaching
assistant, whatever.

416
00:37:30,630 --> 00:37:32,310
[DOROTHEA]
Why does it always take AI?

417
00:37:37,850 --> 00:37:41,290
[DOROTHEA]
I think it's taking me too far. We can start with other things
like people

418
00:37:41,290 --> 00:37:46,650
[DOROTHEA]
need to have access to books and to also information, not just on
TikTok or

419
00:37:46,650 --> 00:37:51,370
[DOROTHEA]
on social media. And that's where we need to start before we
start thinking about

420
00:37:51,450 --> 00:37:56,890
[DOROTHEA]
creating A specific AI teacher for some remote regions of the
world.

421
00:37:59,210 --> 00:38:05,540
[QUENTIN]
And indeed so some people, I suspect might react when they here
today, somebody mentioning

422
00:38:05,540 --> 00:38:10,780
[QUENTIN]
a library that's, well, you have a lot of. It's not efficient.
You have to

423
00:38:10,780 --> 00:38:14,860
[QUENTIN]
go there, you have to look yourself within something. You have to
open a book,

424
00:38:14,860 --> 00:38:19,660
[QUENTIN]
find the right pages. The thing is with learning, and I was also
doing this

425
00:38:19,660 --> 00:38:24,860
[QUENTIN]
episode on the podcast with MIT Prof. Who is focused on exactly
these questions we

426
00:38:24,860 --> 00:38:28,900
[QUENTIN]
were discussing. And he was raising the point that you need some
friction to learn.

427
00:38:28,900 --> 00:38:36,200
[QUENTIN]
You need those basic. You cannot accelerate everything. You have
to spend time and wrestle

428
00:38:36,200 --> 00:38:42,160
[QUENTIN]
with the thing and actually maybe looking for a page, seeing
something is other book

429
00:38:42,320 --> 00:38:48,320
[QUENTIN]
chapters that you didn't expect expose you to new things, make
you learn better and

430
00:38:48,880 --> 00:38:54,280
[QUENTIN]
basically acquire better knowledge. And he did this very nice
experiment where he gave to

431
00:38:54,280 --> 00:38:59,510
[QUENTIN]
his students to solve a problem. ChatGPT other has a low, lower
level. It was

432
00:38:59,510 --> 00:39:05,630
[QUENTIN]
code llama, basically lower level AI Other has traditional,
traditional resources. Everybody could solve the

433
00:39:05,630 --> 00:39:10,910
[QUENTIN]
task. The one with ChatGPT the fastest. And the day after he, he
said this

434
00:39:10,910 --> 00:39:16,070
[QUENTIN]
in the beginning, you'll have an exam on this topic tomorrow. And
the one who

435
00:39:16,070 --> 00:39:20,950
[QUENTIN]
used ChatGPT had the worst grade. And because basically they
could, even if they knew

436
00:39:20,950 --> 00:39:25,990
[QUENTIN]
they, they will have an exam, it's because they took these
shortcuts and they could

437
00:39:25,990 --> 00:39:31,030
[QUENTIN]
not compensate with these tools to properly learn. So these
things in the physical world

438
00:39:31,030 --> 00:39:34,790
[QUENTIN]
are in more traditional ways of learning, are very useful.

439
00:39:34,950 --> 00:39:38,390
[DOROTHEA]
You need to go through the process. I just quote from a blog
article from

440
00:39:38,390 --> 00:39:43,470
[DOROTHEA]
Nicholas Carr that I read recently on like the myth of automated
learning, you cannot

441
00:39:43,470 --> 00:39:48,510
[DOROTHEA]
automate learning. So he says AI is used by high school and
college students to

442
00:39:48,510 --> 00:39:53,870
[DOROTHEA]
complete written assignments to ease or avoid the work of reading
and writing. Puts the

443
00:39:53,870 --> 00:40:02,030
[DOROTHEA]
process of deskilling at education's core. To automate learning
is to subvert learning. It's a

444
00:40:02,030 --> 00:40:05,430
[DOROTHEA]
brilliant piece. I mean, I wish I could read it from A to Z now,

445
00:40:05,510 --> 00:40:11,790
[DOROTHEA]
here, because the way he nails it, you cannot delegate learning,
you cannot outsource learning,

446
00:40:11,790 --> 00:40:15,430
[DOROTHEA]
you cannot automate it. If you haven't done it yourself and your
brain has understood

447
00:40:15,430 --> 00:40:19,800
[DOROTHEA]
the whole thing, you're not going to know it. And that's why also
when you

448
00:40:19,800 --> 00:40:24,320
[DOROTHEA]
know generative AI came first, it was like, now everyone can be a
writer, everyone

449
00:40:24,320 --> 00:40:30,240
[DOROTHEA]
can be a photographer or an artist or a composer, etc. But. And
it was

450
00:40:30,240 --> 00:40:36,040
[DOROTHEA]
seen as creating more equality. But it's different. Like those
people who have already been

451
00:40:36,040 --> 00:40:40,960
[DOROTHEA]
authors, they become even better because they already know, you
know, they know their skills

452
00:40:40,960 --> 00:40:45,590
[DOROTHEA]
and they can use it to become even better. But people who have
never written

453
00:40:45,590 --> 00:40:50,750
[DOROTHEA]
a text, who don't know what a good text is, they won't be able to

454
00:40:50,750 --> 00:40:55,830
[DOROTHEA]
judge the output that the AI serves them. And the same with
pictures. I mean,

455
00:40:55,830 --> 00:40:59,790
[DOROTHEA]
I will not become an artist just because suddenly I would have
access to a

456
00:40:59,790 --> 00:41:06,230
[DOROTHEA]
Danny whatever because I don't know art. So in the end it's like
keeping those

457
00:41:06,230 --> 00:41:10,830
[DOROTHEA]
who have never learned the skills and if they're never taught the
skills anymore, because

458
00:41:10,830 --> 00:41:15,910
[DOROTHEA]
we don't invest into human teachers and into the process of
acquiring skills, it keeps

459
00:41:15,910 --> 00:41:21,370
[DOROTHEA]
them further away and it blocks them from entering the whole
world of art or,

460
00:41:21,370 --> 00:41:26,770
[DOROTHEA]
you know, writing etc, Whereas those who had already, who had the
skills already, the

461
00:41:26,770 --> 00:41:33,570
[DOROTHEA]
moment that generative AI popped up, they're just moving further
ahead and it's really driving

462
00:41:33,570 --> 00:41:34,290
[DOROTHEA]
people apart.

463
00:41:34,850 --> 00:41:40,130
[QUENTIN]
Yeah. So that's really something that I fear with my students.
Indeed it's seeing this,

464
00:41:40,130 --> 00:41:44,050
[QUENTIN]
I mean, if I've learned to code and I used to do it for the

465
00:41:44,050 --> 00:41:48,760
[QUENTIN]
last 20 years, I can now benefit from these tools because I can
code faster.

466
00:41:48,760 --> 00:41:55,800
[QUENTIN]
Just reread, check, adapt quickly. But then it's very hard to
learn if you already

467
00:41:55,800 --> 00:42:01,279
[QUENTIN]
start with this. And same with many skills. I mean my. So
somebody told me

468
00:42:01,279 --> 00:42:06,200
[QUENTIN]
recently that they were writing an email in front of maybe
someone who were 20

469
00:42:06,280 --> 00:42:10,920
[QUENTIN]
years old something and he was surprised. The person was like,
but do you still

470
00:42:10,920 --> 00:42:12,040
[QUENTIN]
write email yourself?

471
00:42:14,120 --> 00:42:18,980
[QUENTIN]
And others will say what's that? I've heard many, many times
these days. Also, why

472
00:42:18,980 --> 00:42:23,460
[QUENTIN]
do you use Google? What? What's the purpose of using Google? It's
really interesting because

473
00:42:23,460 --> 00:42:29,140
[QUENTIN]
it, it shows how fast the world changed and indeed it, it raised
many questions

474
00:42:29,140 --> 00:42:33,500
[QUENTIN]
on. On what? And that's really my perspective today when I teach
because I spend

475
00:42:33,500 --> 00:42:38,940
[QUENTIN]
a lot of time I was doing experimenting but also using AI and
hopefully in

476
00:42:38,940 --> 00:42:43,960
[QUENTIN]
some way that I, I can indeed augment my capacity and not reduce
it. Also

477
00:42:43,960 --> 00:42:48,240
[QUENTIN]
with learning, when I use specific tools like NotebookLM to just
interact with the content

478
00:42:48,240 --> 00:42:52,680
[QUENTIN]
in an efficient way, but still using it like a tutor or a
sparring partner

479
00:42:52,680 --> 00:42:58,520
[QUENTIN]
to acquire the knowledge rather than spitting the answers. And
the thing is, what I

480
00:42:58,520 --> 00:43:04,640
[QUENTIN]
always share is as every time when I use AI, I try to do it

481
00:43:04,800 --> 00:43:10,040
[QUENTIN]
consciously. I know that I might gain some productivity time, at
least on the short

482
00:43:10,040 --> 00:43:14,800
[QUENTIN]
run. But almost inevitably I will lose something.

483
00:43:15,040 --> 00:43:15,520
[DOROTHEA]
Yes.

484
00:43:15,600 --> 00:43:20,040
[QUENTIN]
And I have to accept this and make the conscious decision, okay,
I'm ready to

485
00:43:20,040 --> 00:43:25,440
[QUENTIN]
lose on the learning capacity on this to gain a bit time to focus
on

486
00:43:25,440 --> 00:43:31,640
[QUENTIN]
something that I give more value but. But this trade off is not
always clear

487
00:43:31,640 --> 00:43:36,400
[QUENTIN]
to everyone and I think it's important to take this into account
before interacting.

488
00:43:37,440 --> 00:43:41,120
[DOROTHEA]
Yeah, I think it's a big advantage of those who know both worlds
because we

489
00:43:41,120 --> 00:43:44,760
[DOROTHEA]
can compare. We see what we gain, we see what we lose. But there
is

490
00:43:44,760 --> 00:43:49,480
[DOROTHEA]
also this famous saying like every augmentation is an amputation.
I think it just applies,

491
00:43:49,480 --> 00:43:55,640
[DOROTHEA]
you know, I mean like when you simple things if you only use
Google Maps

492
00:43:55,640 --> 00:44:00,600
[DOROTHEA]
you're not able to read a map anymore, you know how you navigate
etc. And

493
00:44:00,600 --> 00:44:04,440
[DOROTHEA]
this also applies here just that there is more at stake than just
reading a

494
00:44:04,440 --> 00:44:08,400
[DOROTHEA]
map or using Google Maps. There is what is at stake is like as we

495
00:44:08,400 --> 00:44:14,800
[DOROTHEA]
said, like the way of like proper thinking and acquiring
knowledge etc. That's what's at

496
00:44:14,800 --> 00:44:20,120
[DOROTHEA]
stake. So that's. Yeah, but that's why I'm really glad I'm not
involved in. Not

497
00:44:20,120 --> 00:44:25,080
[DOROTHEA]
full time involved in universities et cetera because these
questions are really tough to handle.

498
00:44:25,480 --> 00:44:32,430
[QUENTIN]
They're very hard. Constantly discuss how to evaluate how to
teach. I constantly try in

499
00:44:32,430 --> 00:44:37,190
[QUENTIN]
every class now I teach basically generative AI because it's.
People are using it so

500
00:44:37,190 --> 00:44:41,510
[QUENTIN]
so that's. Anyway yeah try to help it Was this the also the
starting point

501
00:44:41,510 --> 00:44:46,790
[QUENTIN]
with the episode on mental health? The person who created an app
specialized to support

502
00:44:46,869 --> 00:44:52,350
[QUENTIN]
mental health in the university basically based on gen AI he said
well basically one

503
00:44:52,350 --> 00:44:58,830
[QUENTIN]
of and it was a result from a recent research survey. The first
way people

504
00:44:58,830 --> 00:45:05,310
[QUENTIN]
use these gen AI tools recently has been for companionship and
mental health support. So

505
00:45:05,310 --> 00:45:09,310
[QUENTIN]
in a way basically people use it for this. So let's at least give
an

506
00:45:09,310 --> 00:45:15,310
[QUENTIN]
obvious choice that is adequate or at least to the best extent
has some safeguards

507
00:45:15,390 --> 00:45:23,190
[QUENTIN]
and can tweak, push help bring something to the table redirect
people to specialist humans

508
00:45:23,190 --> 00:45:29,280
[QUENTIN]
specialists and and still try to take the advantages from these
tools. For example, in

509
00:45:29,280 --> 00:45:34,240
[QUENTIN]
this the case of mental health we were discussing what happens if
it's 2am and

510
00:45:34,240 --> 00:45:42,280
[QUENTIN]
you cannot reach to anyone and it's just to. You are in a
stressful situation

511
00:45:42,760 --> 00:45:48,480
[QUENTIN]
and you have an adequate tool that can help you with the right
training to

512
00:45:48,480 --> 00:45:54,040
[QUENTIN]
reduce the. The. The mental loads and redirect to. To. To the.
The professional. So

513
00:45:54,040 --> 00:45:59,360
[QUENTIN]
in some situation it can definitely at least I think help if you
take the

514
00:45:59,360 --> 00:46:05,720
[QUENTIN]
advantages. But we have to be very clear that we almost always
lose something.

515
00:46:06,280 --> 00:46:07,720
[DOROTHEA]
Yeah, we do, we do.

516
00:46:08,760 --> 00:46:12,360
[QUENTIN]
So we discuss very quickly about something but again maybe I'm
not sure

517
00:46:14,760 --> 00:46:19,120
[QUENTIN]
how much time you spent discussing and thinking about this. But
about the copyright, which

518
00:46:19,120 --> 00:46:23,020
[QUENTIN]
is very close to the. The ethical questions, right?

519
00:46:23,020 --> 00:46:23,380
[DOROTHEA]
Yes.

520
00:46:23,540 --> 00:46:29,860
[QUENTIN]
We have this basically Ghibli Giblification, and it was something
very dear to my heart

521
00:46:29,860 --> 00:46:33,420
[QUENTIN]
because I was a big, big fan of Ghibli and I grew up with these

522
00:46:33,420 --> 00:46:39,220
[QUENTIN]
movies. And some people were telling me, well, you know, it's,
it's. It's doing some

523
00:46:39,300 --> 00:46:43,620
[QUENTIN]
advertising, some marketing for AO Miyazaki, because I didn't
know him.

524
00:46:43,620 --> 00:46:48,350
[DOROTHEA]
Before, and not what he wants, not the type of advertising he
wishes.

525
00:46:49,140 --> 00:46:54,780
[QUENTIN]
Yeah, exactly. So that was first, also my, my reaction. Second
the second, I was

526
00:46:54,780 --> 00:47:01,260
[QUENTIN]
always also telling these people. So at point taken. Let's say
for Ayao Miyazaki, it's

527
00:47:01,260 --> 00:47:06,020
[QUENTIN]
some kind of advertisements. It increased the number of people
who watch these movies. Recently

528
00:47:06,420 --> 00:47:15,820
[QUENTIN]
I convinced that he would and probably hated it. But let's not
think that I

529
00:47:15,820 --> 00:47:21,060
[QUENTIN]
know the answer. But for all the other artists where you cannot
identify the art,

530
00:47:21,540 --> 00:47:26,380
[QUENTIN]
it's not helping. And this is the vast majority. So you're
already helping the people

531
00:47:26,380 --> 00:47:31,020
[QUENTIN]
on top. So. Yeah, I don't know what you think about this could be
the

532
00:47:31,020 --> 00:47:31,340
[QUENTIN]
fix.

533
00:47:31,340 --> 00:47:37,580
[DOROTHEA]
Yeah. No, no, no, no, no, no. Please don't expect the solution
from me. Just

534
00:47:37,580 --> 00:47:43,060
[DOROTHEA]
today I'm submitting an article for a Swiss art magazine on, on
AI and art

535
00:47:43,060 --> 00:47:48,100
[DOROTHEA]
and copyright and democratization of art, etc. Because I gave a
keynote last year at

536
00:47:48,100 --> 00:47:58,820
[DOROTHEA]
the Institute for Intellectual Property in Switzerland on AI,
creativity and ethics. And so they

537
00:47:58,820 --> 00:48:01,500
[DOROTHEA]
liked it. They said, oh, can we have that, you know, kind of an
article

538
00:48:01,500 --> 00:48:04,740
[DOROTHEA]
on We Need Beer magazine? And we, we want to hear your voice in
the

539
00:48:04,740 --> 00:48:10,060
[DOROTHEA]
magazine. So. Because there, you know, copyright is a, is a, is a
very legal

540
00:48:10,060 --> 00:48:14,260
[DOROTHEA]
issue where. No, I'm not a lawyer. That's the first thing to say.
But the

541
00:48:14,260 --> 00:48:14,900
[DOROTHEA]
thing is that

542
00:48:17,210 --> 00:48:21,770
[DOROTHEA]
as we mentioned before, they have stolen everything, all the
works that they could find,

543
00:48:23,210 --> 00:48:29,930
[DOROTHEA]
whether in violation of copyright or just like in violation of
moral decency. That's another

544
00:48:29,930 --> 00:48:36,090
[DOROTHEA]
question. And then now they're creating tools that are actually
designed in the most cases,

545
00:48:36,490 --> 00:48:44,850
[DOROTHEA]
to actively hide the authority. They're designed to hide the
author. They're designed to avoid

546
00:48:44,930 --> 00:48:50,290
[DOROTHEA]
attribution, they're designed to avoid having to compensate. So
in the Ghibli case, it was

547
00:48:50,290 --> 00:48:55,250
[DOROTHEA]
like in your face, straightforward plagiarism that they were able
to train, like, copy this

548
00:48:55,730 --> 00:49:01,290
[DOROTHEA]
style, which is an exception because usually what we get is like
actively try not

549
00:49:01,290 --> 00:49:09,930
[DOROTHEA]
to look, plagiarize too much and dissolve all the traces of the
individual creativity that

550
00:49:09,930 --> 00:49:16,740
[DOROTHEA]
you have been trained on. And that's the very, very unfair. So
and, and compensation

551
00:49:16,740 --> 00:49:20,820
[DOROTHEA]
models. I don't know if, if I hope that there are so many now,
there

552
00:49:20,820 --> 00:49:26,980
[DOROTHEA]
are so many court cases going on against this practice. I hope
that there will

553
00:49:26,980 --> 00:49:31,860
[DOROTHEA]
be some verdicts where companies are paid, at least in hindsight
or forced to pay

554
00:49:31,860 --> 00:49:38,220
[DOROTHEA]
in hindsight to the creators whose works they have taken. But
it's. It's like after

555
00:49:38,220 --> 00:49:43,730
[DOROTHEA]
the fact, first they stole it and now you know, you cannot undo
the theft.

556
00:49:44,130 --> 00:49:45,170
[DOROTHEA]
It's impossible.

557
00:49:45,490 --> 00:49:50,690
[QUENTIN]
Yeah, indeed. It's already this. And even if you want to,
basically there is this

558
00:49:50,690 --> 00:49:57,650
[QUENTIN]
very important topic and very technical one which is called
machine unlearning. So it's basically

559
00:49:57,650 --> 00:50:02,370
[QUENTIN]
you try to make the machine forget something, it learns, but it's
very hard because

560
00:50:02,370 --> 00:50:08,170
[QUENTIN]
typically with cutting edge models where you have more than 1
trillion parameters, it's very

561
00:50:08,170 --> 00:50:13,430
[QUENTIN]
hard to understand where this is embedded, what's it learned and
to make it forget.

562
00:50:13,430 --> 00:50:18,150
[QUENTIN]
So it's a huge topic now in the engineering basically world, it's
like a.

563
00:50:18,150 --> 00:50:23,430
[DOROTHEA]
Data leakage, privacy, you cannot undo it. It's out there. And
now everything has been

564
00:50:23,430 --> 00:50:31,870
[DOROTHEA]
fed into these models. And now after this has been done, you want
to change

565
00:50:31,870 --> 00:50:33,350
[DOROTHEA]
the reality. It's not possible.

566
00:50:34,390 --> 00:50:37,710
[QUENTIN]
So one thing that I wanted to note that I really appreciate while
talking to

567
00:50:37,710 --> 00:50:43,600
[QUENTIN]
you, which also highlight the difference with ll, is that
several, several times you note

568
00:50:43,600 --> 00:50:47,400
[QUENTIN]
exactly where is your expertise and when you don't want to go,
I'm not a

569
00:50:47,400 --> 00:50:49,320
[QUENTIN]
lawyer, I'm not lawyer.

570
00:50:49,320 --> 00:50:53,040
[DOROTHEA]
I haven't read the paper, I'm not a lawyer, I'm not a machine
learning expert.

571
00:50:53,040 --> 00:50:54,680
[DOROTHEA]
Yeah, I know my boundaries, thanks.

572
00:50:54,840 --> 00:51:00,600
[QUENTIN]
But that's usually. Interestingly, that's something I discuss a
lot over the years. But I

573
00:51:00,600 --> 00:51:07,080
[QUENTIN]
always see people and perceive people more highly instead of the
opposite when I hear

574
00:51:07,500 --> 00:51:12,180
[QUENTIN]
that they openly share their limitations, the things they don't
know because we don't know

575
00:51:12,180 --> 00:51:17,020
[QUENTIN]
everything. And the fact that you're able to know your boundaries
and also to say

576
00:51:17,020 --> 00:51:23,579
[QUENTIN]
it out loud shows that you know that you don't know and your ego
is

577
00:51:24,380 --> 00:51:28,220
[QUENTIN]
not too big such that you're able to say it loudly. Well, this I
don't

578
00:51:28,220 --> 00:51:31,100
[QUENTIN]
know. I let it to the expert to judge. I might have an opinion.

579
00:51:31,100 --> 00:51:35,680
[DOROTHEA]
But no, thanks for mentioning that. And I'm always. I also care
about attribution. You

580
00:51:35,680 --> 00:51:39,720
[DOROTHEA]
know, I want to say where I learned something from because that's
helpful for other

581
00:51:39,720 --> 00:51:43,720
[DOROTHEA]
people to know. I want to say I'm reading from this blog now. I
say,

582
00:51:43,960 --> 00:51:48,919
[DOROTHEA]
you know, I'm reading AI Snake Oil because that's also what
differentiates me from an

583
00:51:48,919 --> 00:51:53,840
[DOROTHEA]
LLM unless it's rag and it's correct. But you know, I said like,
hey, I

584
00:51:53,840 --> 00:51:57,400
[DOROTHEA]
read this in that book. Maybe you want to read that book as well
or

585
00:51:57,400 --> 00:52:02,120
[DOROTHEA]
read that blog piece. And that's what makes us humans. We learn
from humans. And

586
00:52:02,650 --> 00:52:08,330
[DOROTHEA]
I think it's it really. Every single blog piece or article that I
read online

587
00:52:08,570 --> 00:52:16,890
[DOROTHEA]
stays. So much brains ceremons I get from ChatGPT. Even if I only
want to

588
00:52:16,890 --> 00:52:21,650
[DOROTHEA]
know, you know, something brief, it doesn't stick because there
is no human author behind

589
00:52:21,650 --> 00:52:27,250
[DOROTHEA]
it. And I really think there is a difference in reception. When I
read a

590
00:52:27,250 --> 00:52:31,880
[DOROTHEA]
carefully crafted blog post or book book written by humans, it
stays because there is

591
00:52:31,880 --> 00:52:38,280
[DOROTHEA]
a history, there's a story, there's an author, there is
experience behind it. Whereas whatever

592
00:52:38,280 --> 00:52:48,280
[DOROTHEA]
vomit comes through chat on the screen, it's just, you know, I
say stochastic, not

593
00:52:48,520 --> 00:52:54,040
[DOROTHEA]
stochastic gambling. Yeah. Which is plausible, but there's no
authenticity.

594
00:52:54,520 --> 00:53:00,040
[QUENTIN]
Yeah, I really like this, this point about indeed you connect
better due to the

595
00:53:00,120 --> 00:53:05,840
[QUENTIN]
history behind it. Really also relate to the many discussion I
had about art where

596
00:53:05,840 --> 00:53:09,200
[QUENTIN]
people were telling me well, but if you see a piece of art and
you

597
00:53:09,200 --> 00:53:13,200
[QUENTIN]
cannot say that it's AI generated and you find it beautiful,
would it be really

598
00:53:13,200 --> 00:53:17,560
[QUENTIN]
different that if it's made by a human? It is. It is.

599
00:53:17,960 --> 00:53:24,600
[DOROTHEA]
Imagine you read, you read a great book, your favorite novel, and
then you notice

600
00:53:24,600 --> 00:53:29,140
[DOROTHEA]
it has been written by Claude. You will be so disappointed
because you had ideas

601
00:53:29,140 --> 00:53:33,420
[DOROTHEA]
in your mind about the author while you were reading it. You were
creating, you

602
00:53:33,420 --> 00:53:38,980
[DOROTHEA]
know, assumptions, you know, explicitly or implicitly, and then
you find out there is no

603
00:53:38,980 --> 00:53:45,740
[DOROTHEA]
author behind it. There is no story. It's. It's frustrating,
it's. It's disillusioning. It's. I

604
00:53:45,740 --> 00:53:50,340
[DOROTHEA]
call it the disenchantment of, of art through AI.

605
00:53:51,140 --> 00:53:56,630
[QUENTIN]
No. So I 100% agree. Some people disagree and I had many debate
about this,

606
00:53:57,420 --> 00:54:03,620
[QUENTIN]
but it's a very interesting debate. But it feels that I have been
discussing with

607
00:54:03,620 --> 00:54:05,940
[QUENTIN]
you for five minutes and I see that the time is moving.

608
00:54:05,940 --> 00:54:06,780
[DOROTHEA]
Yes, it's running.

609
00:54:06,780 --> 00:54:12,620
[QUENTIN]
There is a huge topic I wanted to discuss which is the impact on
sustainability

610
00:54:12,780 --> 00:54:17,820
[QUENTIN]
and environmental questions. And I'm very curious because so far
I failed to find someone

611
00:54:17,820 --> 00:54:22,180
[QUENTIN]
to come on the podcast and discuss this. It's a very tough one.
So I've

612
00:54:22,180 --> 00:54:29,270
[QUENTIN]
been also working on sustainability before moving away from
academia and measuring environmental impact is

613
00:54:29,270 --> 00:54:37,030
[QUENTIN]
difficult for many reasons. But first I'm curious about what your
view on this in

614
00:54:37,030 --> 00:54:43,750
[QUENTIN]
light, for example, of this recent estimation, which is what it's
worth from Sam Altman

615
00:54:43,750 --> 00:54:52,410
[QUENTIN]
on his blog mentioning that basically an average request on
ChatGPT has a low use,

616
00:54:52,410 --> 00:54:57,010
[QUENTIN]
very low energy, more or less, I'm not sure now, but one minute
of the

617
00:54:57,010 --> 00:55:02,770
[QUENTIN]
oven, that is all these comparisons, like, yeah, some, something
drive your car.

618
00:55:02,770 --> 00:55:07,770
[DOROTHEA]
It's worse than asking ChatGPT if you, you know, whatever. All
the, like, comparing Apple

619
00:55:07,770 --> 00:55:12,130
[DOROTHEA]
and oranges is like one of the favorite disciplines of people in
the field. Yeah.

620
00:55:12,130 --> 00:55:16,130
[DOROTHEA]
I must say, you know, again, I'm not an engineer. So now Sam
Altman said

621
00:55:16,130 --> 00:55:22,130
[DOROTHEA]
it's, it's 0.37 kilowatts, you know, one query. But then again,
you know, the methodological

622
00:55:22,130 --> 00:55:27,090
[DOROTHEA]
discussions start, oh, this is only for text. He didn't talk
about image generation. And,

623
00:55:27,250 --> 00:55:30,490
[DOROTHEA]
and you know what, if you multiply it by billions of times, of
course you

624
00:55:30,490 --> 00:55:36,330
[DOROTHEA]
can say, oh, One query with ChatGPT is much, much better or less
worse than

625
00:55:36,330 --> 00:55:40,770
[DOROTHEA]
driving my car 10 kilometers. But, you know, it's, it's a scale
that accounts for

626
00:55:40,770 --> 00:55:47,520
[DOROTHEA]
it. I think the problem is that the coincidence, the fact that at
a time

627
00:55:47,600 --> 00:55:55,240
[DOROTHEA]
when, let's say in 2020 or like, yeah, when the pandemic hit, et
cetera, we

628
00:55:55,240 --> 00:56:00,080
[DOROTHEA]
were on a path where there seemed to be some consensus that, you
know, climate

629
00:56:00,080 --> 00:56:03,480
[DOROTHEA]
change is an issue that we should tackle and we need, you know,
become more

630
00:56:03,480 --> 00:56:10,270
[DOROTHEA]
energy efficient, we need to have renewable energies. And then
suddenly several things happen. One

631
00:56:10,270 --> 00:56:17,710
[DOROTHEA]
of them being generative AI pops up and is so energy intense that
it threatens

632
00:56:18,190 --> 00:56:25,150
[DOROTHEA]
the transformation, the energy transformation. And that
generative AI or transform models, like the big

633
00:56:26,110 --> 00:56:32,950
[DOROTHEA]
AI models, they are responsible for data centers in areas with
water stress, where the

634
00:56:32,950 --> 00:56:38,400
[DOROTHEA]
data centers consume a lot of water and a lot of electricity,
etc. And that's

635
00:56:38,400 --> 00:56:43,280
[DOROTHEA]
just. It couldn't have come at a worse time because just at a
time when

636
00:56:43,280 --> 00:56:48,920
[DOROTHEA]
we kind of agreed that, you know, we should move towards
renewables, et cetera, this

637
00:56:48,920 --> 00:56:53,840
[DOROTHEA]
pops up. And it's also very interesting from business ethics
point of view, to see

638
00:56:53,840 --> 00:56:57,360
[DOROTHEA]
the change in the tech industry, as you said at the beginning of
this podcast,

639
00:56:58,240 --> 00:57:03,200
[DOROTHEA]
how the CEOs from Big Tech lined up at Trump's inauguration a few
years before.

640
00:57:03,280 --> 00:57:08,340
[DOROTHEA]
They were like, saying things like, Microsoft would say, we are
becoming carbon negative by

641
00:57:08,340 --> 00:57:16,060
[DOROTHEA]
2030, we will remove all historical emissions since 1975. And
then they partially bought OpenAI,

642
00:57:16,460 --> 00:57:21,740
[DOROTHEA]
and now they're falling behind their climate targets. And what
for, you know, if, if

643
00:57:21,980 --> 00:57:27,060
[DOROTHEA]
the kind of AI that OpenAI develops at least was the same kind of
AI

644
00:57:27,060 --> 00:57:31,900
[DOROTHEA]
that can help us tackle climate change issues then it would be
fair. But the

645
00:57:31,900 --> 00:57:37,870
[DOROTHEA]
kind of AI that OpenAI develops is mostly used for Ghibli fraud,
for you know,

646
00:57:37,870 --> 00:57:43,350
[DOROTHEA]
fake chat or fake cat videos or deep fake porn, whatever, you
know, that's consuming

647
00:57:43,350 --> 00:57:50,550
[DOROTHEA]
energy. Whereas potential solutions to climate change issues
probably come from far less energy intense

648
00:57:50,550 --> 00:57:54,750
[DOROTHEA]
models. And this is, I don't know, I can't get my head around
that and

649
00:57:55,790 --> 00:58:00,710
[DOROTHEA]
how we try to absolve ourselves from the responsibility one query
only uses, you know

650
00:58:00,710 --> 00:58:02,430
[DOROTHEA]
that and that much and well,

651
00:58:04,510 --> 00:58:07,390
[DOROTHEA]
I won't use my car today so I can spend my day on chat GPT

652
00:58:07,630 --> 00:58:13,150
[DOROTHEA]
with a good conscience. That's just ridiculous. It's. We really
need to think about that.

653
00:58:14,590 --> 00:58:19,670
[QUENTIN]
And, and basically you point exactly to the direction why it's
also very complex to

654
00:58:19,670 --> 00:58:26,270
[QUENTIN]
answer these questions because in the narrative of AGI, one of
the arguments connected to

655
00:58:26,270 --> 00:58:33,600
[QUENTIN]
sustainability is the following. What if it can helps to solve
cancer or, or climate

656
00:58:33,600 --> 00:58:38,400
[QUENTIN]
change? Because you find the solutions with these tools, what is
the cost you're ready

657
00:58:38,400 --> 00:58:45,360
[QUENTIN]
to pay to basically solve these issues? Which is. It's a very
difficult question to

658
00:58:45,360 --> 00:58:50,200
[QUENTIN]
to answer because you have a lot of uncertainty the capacity of
these tools to

659
00:58:50,200 --> 00:58:57,210
[QUENTIN]
reach this goal and seconds when and if, if it will be capable of
doing

660
00:58:57,210 --> 00:59:02,410
[QUENTIN]
this. So then you investing with a lot of uncertainty. But that
being said, it's

661
00:59:02,410 --> 00:59:08,290
[QUENTIN]
still true that we use these tools in scientific research
extensively. If it's helping or

662
00:59:08,290 --> 00:59:14,050
[QUENTIN]
not, we can leave it open here and hopefully it has at least both
effects.

663
00:59:14,050 --> 00:59:18,970
[QUENTIN]
In some situation might help, in other it might be destructive.
But then basically it

664
00:59:18,970 --> 00:59:24,670
[QUENTIN]
leads to the right concept comparison and I think that's the
hardest because indeed you

665
00:59:24,670 --> 00:59:31,470
[QUENTIN]
create some new needs and energy spending like giblification.
Everybody create these images that are

666
00:59:31,470 --> 00:59:38,870
[QUENTIN]
energy consuming using water and that maybe people who have done
other activities less harmful

667
00:59:38,870 --> 00:59:43,430
[QUENTIN]
to the environment without I'm not sure. But when it comes to
work and also

668
00:59:44,710 --> 00:59:50,600
[QUENTIN]
how we substitute some actions and gain in some situation
productivity, we still have this

669
00:59:50,600 --> 00:59:56,440
[QUENTIN]
question what will be the static code without Genai, without AI
in general? What will

670
00:59:56,440 --> 01:00:00,680
[QUENTIN]
be the action the human will take and will it be more or less
harmful

671
01:00:00,680 --> 01:00:05,800
[QUENTIN]
to the environment because humans are polluting, right? And if I
think that I have

672
01:00:05,800 --> 01:00:13,320
[QUENTIN]
to, I don't know, write a report doing research on Google,
writing and writing and

673
01:00:13,320 --> 01:00:18,200
[QUENTIN]
writing on my computer and taking days. If I can be a bit more
efficient

674
01:00:18,200 --> 01:00:24,000
[QUENTIN]
with Gen AI spending more energy but for shorter amount of time
it's difficult for

675
01:00:24,000 --> 01:00:28,760
[QUENTIN]
me to wrap my head around this. So that's why I still struggle to
fully

676
01:00:28,920 --> 01:00:34,000
[QUENTIN]
have a position on this for sure. You see the data centers they
suggest to

677
01:00:34,000 --> 01:00:34,280
[QUENTIN]
create

678
01:00:36,600 --> 01:00:42,210
[QUENTIN]
nuclear facilities next door just to fuel with LGDs. So for sure
it's, it's, it's

679
01:00:42,210 --> 01:00:48,650
[QUENTIN]
a massive. And I, I agree that reducing to the average cost of a
request

680
01:00:48,650 --> 01:00:57,170
[QUENTIN]
is really misleading because you have complex, more or less
complex requests and also vastly

681
01:00:57,170 --> 01:01:02,370
[QUENTIN]
different type of usage. Some that are relatively useless to
compared to other that might

682
01:01:02,370 --> 01:01:07,090
[QUENTIN]
help to do scientific research. And we can, we can also debate if
actually art

683
01:01:07,090 --> 01:01:12,060
[QUENTIN]
on no, not art but just joking and create these things are, is
useless or

684
01:01:12,060 --> 01:01:15,900
[QUENTIN]
not. But well just overall as you can see, I'm a bit puzzled by
this

685
01:01:15,900 --> 01:01:16,900
[QUENTIN]
question. I'm not sure.

686
01:01:16,980 --> 01:01:21,300
[DOROTHEA]
Yeah, same same. I mean I'm hoping, you know, I'm hoping it's
true. I'm, I'm

687
01:01:21,300 --> 01:01:24,900
[DOROTHEA]
hoping that it pays off. I'm hoping that this is, you know,
what's going to

688
01:01:24,900 --> 01:01:28,340
[DOROTHEA]
happen, that we're having a breakthrough. But still, as I said a
few minutes ago,

689
01:01:28,660 --> 01:01:34,420
[DOROTHEA]
it takes humans to cooperate. Even if AI finds a puzzling
solution to cancer or

690
01:01:34,420 --> 01:01:39,200
[DOROTHEA]
you know, climate change, whatever, are we willing to accept that
or is the, you

691
01:01:39,200 --> 01:01:43,320
[DOROTHEA]
know, we also have an oil industry and fossil interests that are
not in favor

692
01:01:43,320 --> 01:01:44,000
[DOROTHEA]
of that. So.

693
01:01:44,000 --> 01:01:52,079
[QUENTIN]
Yeah, so what will be your advice at the individual level already
for people who

694
01:01:52,079 --> 01:01:55,000
[QUENTIN]
want to interact with Genai in a more ethical way?

695
01:01:58,200 --> 01:02:04,040
[DOROTHEA]
Use it carefully. Well, I mean what can I say? You can choose the
provider.

696
01:02:04,120 --> 01:02:12,720
[DOROTHEA]
We do have European alternatives now with Lucia like the, the
French, France based AI.

697
01:02:12,720 --> 01:02:17,440
[DOROTHEA]
So if you have some geopolitical issues and if you, you know,
care about these

698
01:02:17,440 --> 01:02:24,800
[DOROTHEA]
topics, you have European alternatives. You know, make sure that
you never copy paste, make

699
01:02:24,800 --> 01:02:29,600
[DOROTHEA]
sure that you, you know, keep your brain in the loop. But you
know, as

700
01:02:29,600 --> 01:02:36,920
[DOROTHEA]
I said in the beginning, it's yeah, all the educational advice is
very difficult to

701
01:02:36,920 --> 01:02:44,240
[DOROTHEA]
give and I'm also still figuring out how to use it best myself.
Haven't resolved

702
01:02:44,240 --> 01:02:44,680
[DOROTHEA]
that yet.

703
01:02:45,960 --> 01:02:51,920
[QUENTIN]
And for businesses, will you have also advices or things for
businesses?

704
01:02:51,920 --> 01:02:57,120
[DOROTHEA]
I think you know, your, the way how you handle AI, where you use
it,

705
01:02:57,120 --> 01:03:02,960
[DOROTHEA]
how you use it is part of your overall non financial
responsibility. Just as you

706
01:03:02,960 --> 01:03:09,200
[DOROTHEA]
address your environmental responsibility, your social
responsibility, you have a technological responsibility, make
sure you

707
01:03:10,320 --> 01:03:17,200
[DOROTHEA]
include AI into your materiality assessments. Is this where do we
have an impact with

708
01:03:17,200 --> 01:03:21,770
[DOROTHEA]
our use of AI, what are the risks, etc. You know, just deal with
it

709
01:03:21,930 --> 01:03:25,810
[DOROTHEA]
like one of the other issues that you have already had to
incorporate into your

710
01:03:25,810 --> 01:03:28,010
[DOROTHEA]
non financial responsibility strategies.

711
01:03:29,530 --> 01:03:34,450
[QUENTIN]
And one very last question that I love to ask to close the
podcast. Are

712
01:03:34,450 --> 01:03:37,530
[QUENTIN]
you more hopeful or fearful about the future with AI?

713
01:03:39,610 --> 01:03:41,210
[DOROTHEA]
Can I say I'm Swiss, I'm neutral?

714
01:03:44,330 --> 01:03:49,450
[DOROTHEA]
I don't know. I don't know. It's really difficult because it's on
so many levels.

715
01:03:49,530 --> 01:03:53,670
[DOROTHEA]
On a personal level, I think for, for me, you know, I, I might be

716
01:03:53,670 --> 01:03:58,470
[DOROTHEA]
one of those who benefit. I don't feel threatened, but on a, on a
global

717
01:03:58,470 --> 01:04:03,470
[DOROTHEA]
scale, it looks different. So I can't really have one answer to
that question. It

718
01:04:03,470 --> 01:04:08,430
[DOROTHEA]
really depends on whose perspectives I might benefit, I guess, or
overall, I, I'm not

719
01:04:08,430 --> 01:04:14,790
[DOROTHEA]
threatened by it, but for other people it looks different. And
we, as we said

720
01:04:14,790 --> 01:04:20,310
[DOROTHEA]
with the Sustainable Development Goals, we said we leave no one
behind. And I think,

721
01:04:20,310 --> 01:04:25,300
[DOROTHEA]
you know, the same has to be, you know, applied to AI. We need to

722
01:04:25,300 --> 01:04:29,780
[DOROTHEA]
make sure that we don't just cut off people who never had a
chance to

723
01:04:29,780 --> 01:04:32,940
[DOROTHEA]
even acquire skills and leave them behind.

